{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143e6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "dev = json.load(open(\"../data/individual_dev_end2end_final.json\"))\n",
    "\n",
    "get_lemma = lambda x:\" \".join([token.lemma_ for token in nlp(x)])\n",
    "\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "relation_trigger = ['before', 'after', 'during', 'while']\n",
    "\n",
    "warmup_qs = ['What will happen in the future?', 'What event has already finished?', \n",
    "             'What event has begun but has not finished?', 'What is happening now?',\n",
    "             'What event has already happened?', 'What event has started?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8239e8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1483/1483 [1:18:13<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_results = {}\n",
    "for key, item in tqdm(dev.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"}\\nA:\"\n",
    "    \n",
    "    zero_shot_results[key] = openai.Completion.create(\n",
    "              model=\"text-davinci-003\",\n",
    "              prompt=prompt,\n",
    "              max_tokens=256,\n",
    "              temperature=0\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58400258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1483it [00:00, 25740.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1483 questions\n",
      "Eval on the current eval positive class Micro F1 (Agg) is: 0.4179\n",
      "Eval on the current eval positive class Macro F1 (Relaxed) is: 0.4590\n",
      "Eval on the current eval positive class Macro F1 (Agg) is: 0.3806\n",
      "Eval on the current eval exact match (Agg) ratio is: 0.0593\n",
      "Eval on the current eval exact match ratio (Relaxed) is: 0.0816\n",
      "Eval on 485 Clusters\n",
      "Eval on the current eval clustered EM (Agg) is: 0.0041\n",
      "Eval on the current eval clustered EM (Relaxed) is: 0.0041\n",
      "Eval on the current eval clusrered F1 (max>=0.8) is: 0.0124\n"
     ]
    }
   ],
   "source": [
    "from eval_func_gpt3 import evaluate\n",
    "evaluate(zero_shot_results, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49132fe0",
   "metadata": {},
   "source": [
    "# one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b54bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"../data/train_end2end_final.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5352777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docid_AFP_ENG_19970401.0534_sentid_6_39DD6S19JQ0Q8KWDBW0CSU5CC44ZE7_3\n",
      "{'answers': {'labels': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'types': [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, 'context': ['A', 'public', 'security', 'bureau', 'official', 'said', 'the', 'police', 'were', 'unable', 'to', 'comment', 'on', 'the', 'statement', '.', 'But', 'Shanghai', 'police', 'did', 'say', 'that', 'on', 'March', '24', 'they', 'had', 'taken', 'similar', 'action', 'against', 'Bishop', 'Joseph', 'Fan', 'Zhongliang', ',', 'coadjutor', 'of', 'the', 'underground', 'Roman', 'Catholic', 'Church', '.'], 'question': 'What happened prior to the public security bureau official speaking?'}\n",
      "Given the context A public security bureau official said the police were unable to comment on the statement . But Shanghai police did say that on March 24 they had taken similar action against Bishop Joseph Fan Zhongliang , coadjutor of the underground Roman Catholic Church ., What happened prior to the public security bureau official speaking?, select none or several from {said, were, say, taken\n",
      "A: were, taken\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "random_key = np.random.choice(list(train.keys()), 1)[0]\n",
    "print(random_key)\n",
    "item = train[random_key]\n",
    "print(item)\n",
    "\n",
    "context = \" \".join(item['context'])\n",
    "question = item['question']\n",
    "all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "# examplar_prompt_0 = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + f\"\\nA: {', '.join(ground)}\"\n",
    "examplar_prompt_0 = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + f\"\\nA: {', '.join(ground)}\"\n",
    "print(examplar_prompt_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933119be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1483/1483 [50:09<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "one_shot_results_0 = {}\n",
    "for key, item in tqdm(dev.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"}\\nA:\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            one_shot_results_0[key] = openai.Completion.create(\n",
    "                      model=\"text-davinci-003\",\n",
    "                      prompt=examplar_prompt_0 + \"\\n\\n\" + prompt,\n",
    "                      max_tokens=256,\n",
    "                      temperature=0\n",
    "            )[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce172350",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/template_2_oneshot_exemplar_0.txt', 'w') as writer:\n",
    "    writer.writelines(examplar_prompt_0)\n",
    "with open('results/template_2_oneshot_0_pred', 'w') as writer:\n",
    "    json.dump(one_shot_results_0, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97bf0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1483it [00:00, 38033.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1483 questions\n",
      "Eval on the current eval positive class Micro F1 (Agg) is: 0.4620\n",
      "Eval on the current eval positive class Macro F1 (Relaxed) is: 0.5048\n",
      "Eval on the current eval positive class Macro F1 (Agg) is: 0.4261\n",
      "Eval on the current eval exact match (Agg) ratio is: 0.0668\n",
      "Eval on the current eval exact match ratio (Relaxed) is: 0.0951\n",
      "Eval on 485 Clusters\n",
      "Eval on the current eval clustered EM (Agg) is: 0.0041\n",
      "Eval on the current eval clustered EM (Relaxed) is: 0.0041\n",
      "Eval on the current eval clusrered F1 (max>=0.8) is: 0.0247\n"
     ]
    }
   ],
   "source": [
    "evaluate(one_shot_results_0, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d3e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docid_wsj_0791_sentid_7_3N8OEVH1FV4K14LQJ53CXLQVB2SOOE_1\n",
      "{'answers': {'labels': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'types': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]}, 'context': ['Norwood', 'is', 'controlled', 'by', 'Daniel', 'L.', 'Barnett', 'and', 'Paul', 'A.', 'Reese', ',', 'both', 'officers', 'of', 'Boston-based', 'Oasis', 'Capital', 'Management', 'Inc.', ',', 'a', 'small', 'Boston', 'money', 'management', 'firm', '.', 'Also', 'involved', 'in', 'the', 'group', 'is', 'Robert', 'F.', 'Angelo', ',', 'formerly', 'Phoenix', \"'s\", 'senior', 'vice', 'president', ',', 'field', 'operations', ',', 'who', 'left', 'Phoenix', 'at', 'the', 'beginning', 'of', 'October', '.'], 'question': 'What is happening now?'}\n",
      "Given the context Norwood is controlled by Daniel L. Barnett and Paul A. Reese , both officers of Boston-based Oasis Capital Management Inc. , a small Boston money management firm . Also involved in the group is Robert F. Angelo , formerly Phoenix 's senior vice president , field operations , who left Phoenix at the beginning of October ., What is happening now?, select none or several from {controlled, involved, left\n",
      "A: controlled\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "random_key = np.random.choice(list(train.keys()), 1)[0]\n",
    "print(random_key)\n",
    "item = train[random_key]\n",
    "print(item)\n",
    "\n",
    "context = \" \".join(item['context'])\n",
    "question = item['question']\n",
    "all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "# examplar_prompt_0 = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + f\"\\nA: {', '.join(ground)}\"\n",
    "examplar_prompt_1 = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + f\"\\nA: {', '.join(ground)}\"\n",
    "print(examplar_prompt_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04a5184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1483/1483 [47:57<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "one_shot_results_1 = {}\n",
    "for key, item in tqdm(dev.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"}\\nA:\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            one_shot_results_1[key] = openai.Completion.create(\n",
    "                      model=\"text-davinci-003\",\n",
    "                      prompt=examplar_prompt_1 + \"\\n\\n\" + prompt,\n",
    "                      max_tokens=256,\n",
    "                      temperature=0\n",
    "            )[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb765233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1483it [00:00, 52636.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1483 questions\n",
      "Eval on the current eval positive class Micro F1 (Agg) is: 0.4654\n",
      "Eval on the current eval positive class Macro F1 (Relaxed) is: 0.5127\n",
      "Eval on the current eval positive class Macro F1 (Agg) is: 0.4348\n",
      "Eval on the current eval exact match (Agg) ratio is: 0.0931\n",
      "Eval on the current eval exact match ratio (Relaxed) is: 0.1261\n",
      "Eval on 485 Clusters\n",
      "Eval on the current eval clustered EM (Agg) is: 0.0082\n",
      "Eval on the current eval clustered EM (Relaxed) is: 0.0082\n",
      "Eval on the current eval clusrered F1 (max>=0.8) is: 0.0268\n"
     ]
    }
   ],
   "source": [
    "evaluate(one_shot_results_1, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43535c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/template_2_oneshot_exemplar_1.txt', 'w') as writer:\n",
    "    writer.writelines(examplar_prompt_1)\n",
    "with open('results/template_2_oneshot_1_pred', 'w') as writer:\n",
    "    json.dump(one_shot_results_1, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d08ed7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docid_AFP_ENG_20051212.0486_sentid_6_3TYCR1GOTD84EQ65SING5PVWKW6ZLK_2\n",
      "{'answers': {'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'types': [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0]}, 'context': ['About', '59', 'percent', 'of', 'eligible', 'Iraqis', 'cast', 'ballots', 'last', 'January', 'for', 'an', 'interim', 'parliament', ',', 'and', '64', 'percent', 'voted', 'in', 'the', 'October', '15', 'referendum', 'on', 'a', 'new', 'constitution', '.', 'The', 'US', 'envoy', 'said', 'that', 'unlike', 'the', 'runup', 'to', 'January', \"'s\", 'ballot', ',', '@', 'the', 'level', 'of', 'violence', 'has', 'dropped', 'my', 'almost', 'any', 'measure', 'over', 'the', 'past', 'few', 'days', 'and', 'we', 'hope', 'that', 'that', 'trend', 'continues', '.', '@'], 'question': 'What will happen in the future?'}\n",
      "Given the context About 59 percent of eligible Iraqis cast ballots last January for an interim parliament , and 64 percent voted in the October 15 referendum on a new constitution . The US envoy said that unlike the runup to January 's ballot , @ the level of violence has dropped my almost any measure over the past few days and we hope that that trend continues . @, What will happen in the future?, select none or several from {cast, ballots, voted, referendum, said, runup, ballot, violence, dropped, hope, trend, continues\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "random_key = np.random.choice(list(train.keys()), 1)[0]\n",
    "print(random_key)\n",
    "item = train[random_key]\n",
    "print(item)\n",
    "\n",
    "context = \" \".join(item['context'])\n",
    "question = item['question']\n",
    "all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "# examplar_prompt_0 = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + f\"\\nA: {', '.join(ground)}\"\n",
    "examplar_prompt_2 = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + f\"\\nA: {', '.join(ground)}\"\n",
    "print(examplar_prompt_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d350127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1483/1483 [50:16<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "one_shot_results_2 = {}\n",
    "for key, item in tqdm(dev.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"}\\nA:\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            one_shot_results_2[key] = openai.Completion.create(\n",
    "                      model=\"text-davinci-003\",\n",
    "                      prompt=examplar_prompt_2 + \"\\n\\n\" + prompt,\n",
    "                      max_tokens=256,\n",
    "                      temperature=0\n",
    "            )[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea327345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1483it [00:00, 40147.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1483 questions\n",
      "Eval on the current eval positive class Micro F1 (Agg) is: 0.4353\n",
      "Eval on the current eval positive class Macro F1 (Relaxed) is: 0.4885\n",
      "Eval on the current eval positive class Macro F1 (Agg) is: 0.4094\n",
      "Eval on the current eval exact match (Agg) ratio is: 0.0755\n",
      "Eval on the current eval exact match ratio (Relaxed) is: 0.1052\n",
      "Eval on 485 Clusters\n",
      "Eval on the current eval clustered EM (Agg) is: 0.0041\n",
      "Eval on the current eval clustered EM (Relaxed) is: 0.0082\n",
      "Eval on the current eval clusrered F1 (max>=0.8) is: 0.0206\n"
     ]
    }
   ],
   "source": [
    "evaluate(one_shot_results_2, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "304d6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/template_2_oneshot_exemplar_2.txt', 'w') as writer:\n",
    "    writer.writelines(examplar_prompt_2)\n",
    "with open('results/template_2_oneshot_2_pred', 'w') as writer:\n",
    "    json.dump(one_shot_results_2, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4524222",
   "metadata": {},
   "source": [
    "# threeshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"../data/train_end2end_final.json\"))\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "random_keys = np.random.choice(list(train.keys()), 3)\n",
    "print(random_keys)\n",
    "items = [train[k] for k in random_keys]\n",
    "print(items)\n",
    "\n",
    "\n",
    "examplar_list = []\n",
    "for item in items:\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    if len(ground) == 0:\n",
    "        ground = ['none']\n",
    "    tmp_prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + f\"\\nA: {', '.join(ground)}\"\n",
    "    examplar_list.append(tmp_prompt)\n",
    "examplar_threeshot_0 = \"\\n\\n\".join(examplar_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45024199",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_shot_results_0 = {}\n",
    "for key, item in tqdm(dev.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"}\\nA:\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            three_shot_results_0[key] = openai.Completion.create(\n",
    "                      model=\"text-davinci-003\",\n",
    "                      prompt=examplar_threeshot_0 + \"\\n\\n\" + prompt,\n",
    "                      max_tokens=256,\n",
    "                      temperature=0\n",
    "            )[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670af373",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"../data/train_end2end_final.json\"))\n",
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "\n",
    "random_keys = np.random.choice(list(train.keys()), 3)\n",
    "print(random_keys)\n",
    "items = [train[k] for k in random_keys]\n",
    "print(items)\n",
    "\n",
    "\n",
    "examplar_list = []\n",
    "for item in items:\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    if len(ground) == 0:\n",
    "        ground = ['none']\n",
    "    tmp_prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + f\"\\nA: {', '.join(ground)}\"\n",
    "    examplar_list.append(tmp_prompt)\n",
    "examplar_threeshot_1 = \"\\n\\n\".join(examplar_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_shot_results_1 = {}\n",
    "for key, item in tqdm(dev.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"}\\nA:\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            three_shot_results_1[key] = openai.Completion.create(\n",
    "                      model=\"text-davinci-003\",\n",
    "                      prompt=examplar_threeshot_1 + \"\\n\\n\" + prompt,\n",
    "                      max_tokens=256,\n",
    "                      temperature=0\n",
    "            )[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2339639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"../data/train_end2end_final.json\"))\n",
    "import numpy as np\n",
    "np.random.seed(3)\n",
    "\n",
    "random_keys = np.random.choice(list(train.keys()), 3)\n",
    "print(random_keys)\n",
    "items = [train[k] for k in random_keys]\n",
    "print(items)\n",
    "\n",
    "\n",
    "examplar_list = []\n",
    "for item in items:\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    if len(ground) == 0:\n",
    "        ground = ['none']\n",
    "    tmp_prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + f\"\\nA: {', '.join(ground)}\"\n",
    "    examplar_list.append(tmp_prompt)\n",
    "examplar_threeshot_2 = \"\\n\\n\".join(examplar_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b99b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_shot_results_2 = {}\n",
    "for key, item in tqdm(dev.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    prompt = f\"Given the context {context}, {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"}\\nA:\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            three_shot_results_2[key] = openai.Completion.create(\n",
    "                      model=\"text-davinci-003\",\n",
    "                      prompt=examplar_threeshot_2 + \"\\n\\n\" + prompt,\n",
    "                      max_tokens=256,\n",
    "                      temperature=0\n",
    "            )[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
