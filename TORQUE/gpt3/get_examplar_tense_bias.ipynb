{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd1d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "dev = json.load(open(\"../data/individual_dev_end2end_final.json\"))\n",
    "\n",
    "get_lemma = lambda x:\" \".join([token.lemma_ for token in nlp(x)])\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "relation_trigger = ['before', 'after', 'during', 'while']\n",
    "\n",
    "warmup_qs = ['What will happen in the future?', 'What event has already finished?', \n",
    "             'What event has begun but has not finished?', 'What is happening now?',\n",
    "             'What event has already happened?', 'What event has started?', \n",
    "            ]\n",
    "\n",
    "def parse_question(q, event_lemmas):\n",
    "    \"\"\"\n",
    "        input: q: question, events: the set of lemmatized events.\n",
    "        output: \n",
    "            q_events: events in the question\n",
    "            modality: whether there's \"might/will/can/may/...\"\n",
    "            base_temp_rel: basic temporal relations, [\"before\", \"after\", \"during\", \"while\"]\n",
    "    \"\"\"\n",
    "    # acquire the events in the question stem\n",
    "    q_events = [e for e in [token.lemma_ for token in nlp(q)] if e in event_lemmas]\n",
    "    \n",
    "    second_prefix = q.split()[1]\n",
    "\n",
    "    rel_trigger = [t for t in q.split() if t in relation_trigger]\n",
    "\n",
    "    if len(rel_trigger) > 0:\n",
    "        base_temp_rel = rel_trigger[0]\n",
    "    else:\n",
    "        base_temp_rel = \"\"\n",
    "        \n",
    "    return q_events, second_prefix, base_temp_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba7ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5b1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [11:07<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_results = {}\n",
    "for key, item in tqdm(dev_tense.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    \n",
    "    zero_shot_results[key] = openai.Completion.create(\n",
    "              model=\"text-davinci-003\",\n",
    "              prompt=prompt,\n",
    "              max_tokens=256,\n",
    "              temperature=0\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8eb1dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [00:00, 67106.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 243 questions\n",
      "Eval on the current eval positive class Micro F1 (Agg) is: 0.4699\n",
      "Eval on the current eval positive class Macro F1 (Relaxed) is: 0.5346\n",
      "Eval on the current eval positive class Macro F1 (Agg) is: 0.4544\n",
      "Eval on the current eval exact match (Agg) ratio is: 0.0206\n",
      "Eval on the current eval exact match ratio (Relaxed) is: 0.0412\n",
      "Eval on 157 Clusters\n",
      "Eval on the current eval clustered EM (Agg) is: 0.0255\n",
      "Eval on the current eval clustered EM (Relaxed) is: 0.0446\n",
      "Eval on the current eval clusrered F1 (max>=0.8) is: 0.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(zero_shot_results, dev_tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f4054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9821e9f1",
   "metadata": {},
   "source": [
    "# get examplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78a9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tense = json.load(open(\"../data/dataset_bias_new/individual_dev_tense_relation_bias.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3713124e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [21:41<00:00,  5.36s/it]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "generated_examplars = {}\n",
    "generated_examplars_answers = {}\n",
    "examplar_prompts = {}\n",
    "\n",
    "for key, item in tqdm(dev_tense.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "\n",
    "    event_lemmas = [get_lemma(e) for e in all_events]\n",
    "    \n",
    "    if not question in warmup_qs:\n",
    "        question_events, _, rel = parse_question(question, event_lemmas)\n",
    "        selected_ans = np.random.choice(all_events, min(3, len(all_events)), replace=False)\n",
    "        prompt = f\"Write a story where '{', '.join(selected_ans)}' happened {rel} '{question_events[0]}' within 100 words:\"\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    generated_examplars_answers[key] = selected_ans\n",
    "\n",
    "    generated_examplars[key] = openai.Completion.create(\n",
    "                                          model=\"text-davinci-003\",\n",
    "                                          prompt=prompt,\n",
    "                                          max_tokens=100,\n",
    "                                          temperature=0\n",
    "                                )[\"choices\"][0][\"text\"].strip()\n",
    "    time.sleep(2)\n",
    "    # \n",
    "    question = f\"What happened {rel} {question_events[0]}?\"\n",
    "    context = generated_examplars[key]\n",
    "    gen_answers = generated_examplars_answers[key]\n",
    "    ex_prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA: \" + ', '.join(gen_answers)\n",
    "\n",
    "    examplar_prompts[key] = ex_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0fb009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [11:16<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "oneshot_gen_results = {}\n",
    "for key, item in tqdm(dev_tense.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    \n",
    "    examplar = examplar_prompts[key]\n",
    "    \n",
    "    oneshot_gen_results[key] = openai.Completion.create(\n",
    "              model=\"text-davinci-003\",\n",
    "              prompt=examplar + \"\\n\\n\" + prompt,\n",
    "              max_tokens=256,\n",
    "              temperature=0\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a960f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_func_gpt3 import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f05065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [00:00, 28831.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 243 questions\n",
      "Eval on the current eval positive class Micro F1 (Agg) is: 0.4844\n",
      "Eval on the current eval positive class Macro F1 (Relaxed) is: 0.5606\n",
      "Eval on the current eval positive class Macro F1 (Agg) is: 0.4807\n",
      "Eval on the current eval exact match (Agg) ratio is: 0.0165\n",
      "Eval on the current eval exact match ratio (Relaxed) is: 0.0288\n",
      "Eval on 157 Clusters\n",
      "Eval on the current eval clustered EM (Agg) is: 0.0191\n",
      "Eval on the current eval clustered EM (Relaxed) is: 0.0318\n",
      "Eval on the current eval clusrered F1 (max>=0.8) is: 0.2420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(oneshot_gen_results, dev_tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea079c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save examplars\n",
    "\n",
    "with open(\"data/tense_examplar_gpt3.json\", 'w') as outfile:\n",
    "    json.dump(examplar_prompts, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82fec0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tense_examplar_gpt3_oneshot_pred.json\", 'w') as outfile:\n",
    "    json.dump(oneshot_gen_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053076b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817091d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7599fe6",
   "metadata": {},
   "source": [
    "# get counterfactual examplar w/ gpt3\n",
    "\n",
    "counter-factual ones\n",
    "\n",
    "passage, question, all_events.\n",
    "\n",
    "use gpt3 to predict the relations.\n",
    "\n",
    "E.g., what happened after e1??\n",
    "predicts: e21, e22, ..., e2n\n",
    "\n",
    "then, ask gpt3 to generate those that are not e21, e22, ..., e2n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "618116d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [21:14<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "generated_examplars_cf = {}\n",
    "generated_examplars_answers_cf = {}\n",
    "examplar_prompts_cf = {}\n",
    "\n",
    "for key, item in tqdm(dev_tense.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c.lower() for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c.lower() for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "\n",
    "    event_lemmas = [get_lemma(e) for e in all_events]\n",
    "    \n",
    "    if not question in warmup_qs:\n",
    "        question_events, _, rel = parse_question(question, event_lemmas)\n",
    "        gpt3_answers = [w.lower().strip().replace(\".\", \"\") for w in oneshot_gen_results[key].split(\",\")]\n",
    "        selected_ans = list(set(all_events) - set(gpt3_answers))\n",
    "#         print(len(selected_ans), len(gpt3_answers), len(all_events))\n",
    "#         print(selected_ans, gpt3_answers, all_events)\n",
    "        prompt = f\"Write a story where '{', '.join(selected_ans)}' happened {rel} '{question_events[0]}' within 100 words:\"\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    generated_examplars_answers_cf[key] = selected_ans\n",
    "\n",
    "    generated_examplars_cf[key] = openai.Completion.create(\n",
    "                                          model=\"text-davinci-003\",\n",
    "                                          prompt=prompt,\n",
    "                                          max_tokens=100,\n",
    "                                          temperature=0\n",
    "                                )[\"choices\"][0][\"text\"].strip()\n",
    "    time.sleep(2)\n",
    "    # \n",
    "    question = f\"What happened {rel} {question_events[0]}?\"\n",
    "    context = generated_examplars_cf[key]\n",
    "    gen_answers = generated_examplars_answers_cf[key]\n",
    "    ex_prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA: \" + ', '.join(gen_answers)\n",
    "\n",
    "    examplar_prompts_cf[key] = ex_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "633c1d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [11:22<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "oneshot_gen_cf_results = {}\n",
    "for key, item in tqdm(dev_tense.items()):\n",
    "    context = \" \".join(item['context'])\n",
    "    question = item['question']\n",
    "    all_events = [c for c, t in zip(item['context'], item[\"answers\"][\"types\"]) if t]\n",
    "    ground = [c for c, t in zip(item['context'], item[\"answers\"][\"labels\"]) if t]\n",
    "    prompt = f\"Q: {question}\" + \", select none or several from {\" + ', '.join(all_events) + \"} \\n\" + context + \"\\nA:\"\n",
    "    \n",
    "    examplar = examplar_prompts_cf[key]\n",
    "    \n",
    "    oneshot_gen_cf_results[key] = openai.Completion.create(\n",
    "              model=\"text-davinci-003\",\n",
    "              prompt=examplar + \"\\n\\n\" + prompt,\n",
    "              max_tokens=256,\n",
    "              temperature=0\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8171c113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [00:00, 56723.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 243 questions\n",
      "Eval on the current eval positive class Micro F1 (Agg) is: 0.4957\n",
      "Eval on the current eval positive class Macro F1 (Relaxed) is: 0.5686\n",
      "Eval on the current eval positive class Macro F1 (Agg) is: 0.4980\n",
      "Eval on the current eval exact match (Agg) ratio is: 0.0370\n",
      "Eval on the current eval exact match ratio (Relaxed) is: 0.0453\n",
      "Eval on 157 Clusters\n",
      "Eval on the current eval clustered EM (Agg) is: 0.0382\n",
      "Eval on the current eval clustered EM (Relaxed) is: 0.0573\n",
      "Eval on the current eval clusrered F1 (max>=0.8) is: 0.2420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(oneshot_gen_cf_results, dev_tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bf56167",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tense_examplar_cf_gpt3.json\", 'w') as outfile:\n",
    "    json.dump(examplar_prompts_cf, outfile)\n",
    "with open(\"data/tense_examplar_cf_gpt3_oneshot_pred.json\", 'w') as outfile:\n",
    "    json.dump(oneshot_gen_cf_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a400e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
