{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbdf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python\n",
    "import tqdm\n",
    "import cherrypy\n",
    "import cherrypy_cors\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime \n",
    "import random\n",
    "from matres_reader_with_tense import *\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import DataLoader\n",
    "from util import *\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from model import transformers_mlp_cons\n",
    "from exp import *\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from synonyms import *\n",
    "import pickle\n",
    "from timeline_construct import *\n",
    "from ts import func, ModelWithTemperature\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# datetime object containing current date and time\n",
    "now = datetime.datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "\n",
    "#label_dict={\"SuperSub\": 0, \"SubSuper\": 1, \"Coref\": 2, \"NoRel\": 3}\n",
    "num_dict = {0: \"before\", 1: \"after\", 2: \"equal\", 3: \"vague\"}\n",
    "#def label_to_num(label):\n",
    "#    return label_dict[label]\n",
    "def num_to_label(num):\n",
    "    return num_dict[num]\n",
    "\n",
    "mask_in_input_ids = 0 # note that [MASK] is actually learned through pre-training\n",
    "mask_in_input_mask = 0 # when input is masked through attention, it would be replaced with [PAD]\n",
    "acronym = 0 # using acronym for tense (e.g., pastsimp): 1; else (e.g., past simple): 0\n",
    "t_marker = 1\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "def docTransformerTokenIDs(sentences):\n",
    "    if len(sentences) < 1:\n",
    "        return None\n",
    "    elif len(sentences) == 1:\n",
    "        return sentences[0]['_subword_to_ID']\n",
    "    else:\n",
    "        TokenIDs = sentences[0]['_subword_to_ID']\n",
    "        for i in range(1, len(sentences)):\n",
    "            TokenIDs += sentences[i]['_subword_to_ID'][1:]\n",
    "        return TokenIDs\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    max_len = max([len(f['input_ids']) for f in batch])\n",
    "    input_ids = [f['input_ids'] + [0] * (max_len - len(f['input_ids'])) for f in batch]\n",
    "    if mask_in_input_ids:\n",
    "        input_ids_new = []\n",
    "        for f_id, f in enumerate(input_ids):\n",
    "            for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "                end = batch[f_id]['event_pos_end'][event_id]\n",
    "                for token_id in range(start, end): # needs verification\n",
    "                    f[token_id] = 67\n",
    "            input_ids_new.append(f)\n",
    "        input_ids = input_ids_new\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    input_mask = [[1.0] * len(f['input_ids']) + [0.0] * (max_len - len(f['input_ids'])) for f in batch]\n",
    "    if mask_in_input_mask:\n",
    "        input_mask_new = []\n",
    "        for f_id, f in enumerate(input_mask):\n",
    "            for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "                end = batch[f_id]['event_pos_end'][event_id]\n",
    "                for token_id in range(start, end): # needs verification\n",
    "                    f[token_id] = 0.0\n",
    "            input_mask_new.append(f)\n",
    "        input_mask = input_mask_new\n",
    "    # Updated on May 17, 2022    \n",
    "    input_mask_eo = [[0.0] * max_len for f in batch]\n",
    "    for f_id, f in enumerate(input_mask_eo):\n",
    "        for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "            end = batch[f_id]['event_pos_end'][event_id]\n",
    "            for token_id in range(start, end): # needs verification\n",
    "                f[token_id] = 1.0\n",
    "    # Updated on Jun 14, 2022\n",
    "    input_mask_xbar = [[0.0] * max_len for f in batch]\n",
    "    input_mask_xbar = torch.tensor(input_mask_xbar, dtype=torch.float)\n",
    "    input_mask_eo = torch.tensor(input_mask_eo, dtype=torch.float)\n",
    "    input_mask = torch.tensor(input_mask, dtype=torch.float)\n",
    "    event_pos = [f['event_pos'] for f in batch]\n",
    "    event_pos_end = [f['event_pos_end'] for f in batch]\n",
    "    event_pair = [f['event_pair'] for f in batch]\n",
    "    labels = [f['labels'] for f in batch]\n",
    "    output = (input_ids, input_mask, event_pos, event_pos_end, event_pair, labels, input_mask_eo, input_mask_xbar)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbd7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
