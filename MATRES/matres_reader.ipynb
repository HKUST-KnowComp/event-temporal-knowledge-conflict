{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e3588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from eventseg_getter import *\n",
    "from tense_tagger import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80273e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tense_getter(txt):\n",
    "    return print_parsed_text(txt)\n",
    "\n",
    "space = ' '\n",
    "\n",
    "#model = RobertaModel.from_pretrained('roberta-base')\n",
    "#dir_name = \"/shared/why16gzl/logic_driven/Quizlet/Quizlet_2/LDC2020E20_KAIROS_Quizlet_2_TA2_Source_Data_V1.0/data/ltf/ltf/\"\n",
    "#file_name = \"K0C03N4LR.ltf.xml\"    # Use ltf_reader \n",
    "#dir_name = \"/home1/w/why16gzl/KAIROS/hievents_v2/processed/\"\n",
    "#file_name = \"article-10901.tsvx\"   # Use tsvx_reader\n",
    "\n",
    "# ============================\n",
    "#         PoS Tagging\n",
    "# ============================\n",
    "pos_tags = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\", \"SPACE\"]\n",
    "identity_matrix = np.identity(len(pos_tags))\n",
    "postag_to_OneHot = {}\n",
    "postag_to_OneHot[\"None\"] = np.zeros(len(pos_tags))\n",
    "for (index, item) in enumerate(pos_tags):\n",
    "    postag_to_OneHot[item] = identity_matrix[index]\n",
    "    \n",
    "def postag_2_OneHot(postag):\n",
    "    return postag_to_OneHot[postag]\n",
    "\n",
    "# ===========================\n",
    "#        HiEve Labels\n",
    "# ===========================\n",
    "\n",
    "label_dict={\"SuperSub\": 0, \"SubSuper\": 1, \"Coref\": 2, \"NoRel\": 3}\n",
    "num_dict = {0: \"SuperSub\", 1: \"SubSuper\", 2: \"Coref\", 3: \"NoRel\"}\n",
    "def label_to_num(label):\n",
    "    return label_dict[label]\n",
    "def num_to_label(num):\n",
    "    return num_dict[num]\n",
    "\n",
    "# Padding function, both for huggingface encoded sentences, and for part-of-speech tags\n",
    "def padding(sent, pos = False, max_sent_len = 200):\n",
    "    if pos == False:\n",
    "        one_list = [1] * max_sent_len\n",
    "        one_list[0:len(sent)] = sent\n",
    "        return torch.tensor(one_list, dtype=torch.long)\n",
    "    else:\n",
    "        one_list = [\"None\"] * max_sent_len\n",
    "        one_list[0:len(sent)] = sent\n",
    "        return one_list\n",
    "\n",
    "def transformers_list(content, tokenizer, token_list = None, token_span_SENT = None):\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(transformers_model)    \n",
    "    encoded = tokenizer.encode(content)\n",
    "    # input_ids = torch.tensor(encoded).unsqueeze(0)  # Batch size 1\n",
    "    # outputs = model(input_ids)\n",
    "    # last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    _subwords = []\n",
    "    _subword_to_ID = []\n",
    "    _subwords_no_space = []\n",
    "    for index, i in enumerate(encoded):\n",
    "        r_token = tokenizer.decode([i])\n",
    "        if len(r_token) > 0:\n",
    "            _subword_to_ID.append(i)\n",
    "            _subwords.append(r_token)\n",
    "            if r_token[0] == \" \":\n",
    "                _subwords_no_space.append(r_token[1:])\n",
    "            else:\n",
    "                _subwords_no_space.append(r_token)\n",
    "\n",
    "    _subword_span = tokenized_to_origin_span(content, _subwords_no_space[1:-1]) # w/o <s> and </s>\n",
    "    _subword_map = []\n",
    "    if token_span_SENT is not None:\n",
    "        _subword_map.append(-1) # \"<s>\"\n",
    "        for subword in _subword_span:\n",
    "            _subword_map.append(token_id_lookup(token_span_SENT, subword[0], subword[1]))\n",
    "        _subword_map.append(-1) # \"</s>\" \n",
    "        return _subword_to_ID, _subwords, _subword_span, _subword_map\n",
    "    else:\n",
    "        return _subword_to_ID, _subwords, _subword_span, -1\n",
    "\n",
    "def tokenized_to_origin_span(text, token_list):\n",
    "    token_span = []\n",
    "    pointer = 0\n",
    "    previous_pointer = 0\n",
    "    for token in token_list:\n",
    "        while pointer < len(text):\n",
    "            if token[0] == text[pointer]:\n",
    "                start = pointer\n",
    "                end = start + len(token) - 1\n",
    "                previous_pointer = pointer = end + 1\n",
    "                break\n",
    "            else:\n",
    "                pointer += 1\n",
    "        if pointer < len(text):\n",
    "            token_span.append([start, end])\n",
    "        else:\n",
    "            if previous_pointer < len(text):\n",
    "                # exceeding text length, meaning that a weird character is encountered\n",
    "                token_span.append([end + 1, end + 1])\n",
    "                pointer = previous_pointer\n",
    "            else:\n",
    "                # end of text\n",
    "                token_span.append([start, end])\n",
    "    return token_span\n",
    "\n",
    "def sent_id_lookup(my_dict, start_char, end_char = None):\n",
    "    for sent_dict in my_dict['sentences']:\n",
    "        if end_char is None:\n",
    "            if start_char >= sent_dict['sent_start_char'] and start_char <= sent_dict['sent_end_char']:\n",
    "                return sent_dict['sent_id']\n",
    "        else:\n",
    "            if start_char >= sent_dict['sent_start_char'] and end_char <= sent_dict['sent_end_char']:\n",
    "                return sent_dict['sent_id']\n",
    "\n",
    "def token_id_lookup(token_span_SENT, start_char, end_char):\n",
    "    for index, token_span in enumerate(token_span_SENT):\n",
    "        if start_char >= token_span[0] and end_char <= token_span[1]:\n",
    "            return index\n",
    "\n",
    "def span_SENT_to_DOC(token_span_SENT, sent_start):\n",
    "    token_span_DOC = []\n",
    "    #token_count = 0\n",
    "    for token_span in token_span_SENT:\n",
    "        start_char = token_span[0] + sent_start\n",
    "        end_char = token_span[1] + sent_start\n",
    "        #assert my_dict[\"doc_content\"][start_char] == sent_dict[\"tokens\"][token_count][0]\n",
    "        token_span_DOC.append([start_char, end_char])\n",
    "        #token_count += 1\n",
    "    return token_span_DOC\n",
    "\n",
    "def id_lookup(span_SENT, start_char):\n",
    "    # this function is applicable to huggingface subword or token from ltf/spaCy\n",
    "    # id: start from 0\n",
    "    token_id = -1\n",
    "    for token_span in span_SENT:\n",
    "        token_id += 1\n",
    "        if token_span[0] <= start_char and token_span[1] >= start_char:\n",
    "            return token_id\n",
    "    raise ValueError(\"Nothing is found.\")\n",
    "    return token_id\n",
    "\n",
    "def segment_id_lookup(segments, sent_id):\n",
    "    for i in range(len(segments)):\n",
    "        if sent_id > segments[i] and sent_id <= segments[i+1]:\n",
    "            return i\n",
    "\"\"\"\n",
    "map_tense = {\"futu\": \"Future\",\n",
    "             \"perf\": \"Perfect\",\n",
    "             \"simp\": \"Simple\", \n",
    "             \"cont\": \"Continuous\",\n",
    "             \"past\": \"Past\",\n",
    "             \"pres\": \"Present\",\n",
    "             \"pass\": \"Passive\"\n",
    "            }\n",
    "def tense_mapper(tense):\n",
    "    tense = list(map(''.join, zip(*[iter(tense)]*4))) \n",
    "    return ' '.join([map_tense[i] for i in tense])\n",
    "\"\"\"\n",
    "map_tense = {\"futu\": \"Future\",\n",
    "             \"perf\": \"Perfect\",\n",
    "             #\"simp\": \"\",      # DELETE simp\n",
    "             \"simp\": \"Simple\", # KEEP simp\n",
    "             \"cont\": \"Continuous\",\n",
    "             \"past\": \"Past\",\n",
    "             \"pres\": \"Present\",\n",
    "             \"pass\": \"Passive\",\n",
    "            }\n",
    "def tense_mapper(tense):\n",
    "    tense = list(map(''.join, zip(*[iter(tense)]*4))) \n",
    "    return ' '.join([map_tense[i] for i in tense]).strip()\n",
    "        \n",
    "def tense_finder(tense_list, start_char):\n",
    "    # start_char: sentence level \n",
    "    # e.g., 'She would say the soldiers were hit by a truck.'\n",
    "    # ['would', 'MD', [4, 9], 'futusimp']\n",
    "    # ['say', 'VB', [10, 13], 'futusimp', ...\n",
    "    for verb in tense_list:\n",
    "        if verb[2][0] == start_char:\n",
    "            try:\n",
    "                return [tense_mapper(verb[3]), '['+verb[3]+']'] \n",
    "            # Updated on Oct 29, 2022, because of unknown error\n",
    "            except:\n",
    "                return None\n",
    "            # Updated on Feb 27, 2022\n",
    "    #print(start_char)\n",
    "    #print(tense_list)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc3439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdd_reader(text, event_pos, event_pos_end, tokenizer):\n",
    "    my_dict = {}\n",
    "    my_dict[\"event_dict\"] = {}\n",
    "    my_dict[\"sentences\"] = []\n",
    "    my_dict[\"relation_dict\"] = {}\n",
    "    \n",
    "    # Read tsvx file\n",
    "    my_dict[\"doc_content\"] = text\n",
    "        \n",
    "    for i, pos in enumerate(event_pos):\n",
    "        my_dict[\"event_dict\"][i] = {\"mention\": text[event_pos[i]:event_pos_end[i]], \"start_char\": event_pos[i], \"end_char\": event_pos_end[i]} \n",
    "    \n",
    "    # Split document into sentences\n",
    "    sent_tokenized_text = sent_tokenize(my_dict[\"doc_content\"])\n",
    "    #sent_tokenized_text = []\n",
    "    #tense_res = tense_getter(text)\n",
    "    #for sentence in tense_res['sentences']:\n",
    "    #    sent_tokenized_text.append(sentence[0])\n",
    "        \n",
    "    sent_span = tokenized_to_origin_span(my_dict[\"doc_content\"], sent_tokenized_text)\n",
    "    count_sent = 0\n",
    "    end_pos = [1]\n",
    "    for sent in sent_tokenized_text:\n",
    "        sent_dict = {}\n",
    "        sent_dict[\"sent_id\"] = count_sent\n",
    "        sent_dict[\"content\"] = sent\n",
    "        sent_dict[\"sent_start_char\"] = sent_span[count_sent][0]\n",
    "        sent_dict[\"sent_end_char\"] = sent_span[count_sent][1]\n",
    "        sent_dict[\"tense_list\"] = tense_getter(sent)\n",
    "        \n",
    "        spacy_token = nlp(sent_dict[\"content\"])\n",
    "        sent_dict[\"tokens\"] = []\n",
    "        sent_dict[\"pos\"] = []\n",
    "        # spaCy-tokenized tokens & Part-Of-Speech Tagging\n",
    "        for token in spacy_token:\n",
    "            sent_dict[\"tokens\"].append(token.text)\n",
    "            sent_dict[\"pos\"].append(token.pos_)\n",
    "        sent_dict[\"token_span_SENT\"] = tokenized_to_origin_span(sent, sent_dict[\"tokens\"])\n",
    "        sent_dict[\"token_span_DOC\"] = span_SENT_to_DOC(sent_dict[\"token_span_SENT\"], sent_dict[\"sent_start_char\"])\n",
    "\n",
    "        # huggingface tokenizer\n",
    "        sent_dict[\"_subword_to_ID\"], sent_dict[\"_subwords\"], \\\n",
    "        sent_dict[\"_subword_span_SENT\"], sent_dict[\"_subword_map\"] = \\\n",
    "        transformers_list(sent_dict[\"content\"], tokenizer, sent_dict[\"tokens\"], sent_dict[\"token_span_SENT\"])\n",
    "        \n",
    "        if count_sent == 0:\n",
    "            end_pos.append(len(sent_dict[\"_subword_to_ID\"]))\n",
    "        else:\n",
    "            end_pos.append(end_pos[-1] + len(sent_dict[\"_subword_to_ID\"]) - 1)\n",
    "            \n",
    "        sent_dict[\"_subword_span_DOC\"] = \\\n",
    "        span_SENT_to_DOC(sent_dict[\"_subword_span_SENT\"], sent_dict[\"sent_start_char\"])\n",
    "        \n",
    "        sent_dict[\"_subword_pos\"] = []\n",
    "        for token_id in sent_dict[\"_subword_map\"]:\n",
    "            if token_id == -1 or token_id is None:\n",
    "                sent_dict[\"_subword_pos\"].append(\"None\")\n",
    "            else:\n",
    "                sent_dict[\"_subword_pos\"].append(sent_dict[\"pos\"][token_id])\n",
    "        \n",
    "        my_dict[\"sentences\"].append(sent_dict)\n",
    "        count_sent += 1\n",
    "        \n",
    "    my_dict['end_pos'] = end_pos\n",
    "    # Add sent_id as an attribute of event\n",
    "    for event_id, event_dict in my_dict[\"event_dict\"].items():\n",
    "        my_dict[\"event_dict\"][event_id][\"sent_id\"] = sent_id = \\\n",
    "        sent_id_lookup(my_dict, event_dict[\"start_char\"], event_dict[\"end_char\"])\n",
    "        #my_dict[\"event_dict\"][event_id][\"token_id\"] = \\\n",
    "        #id_lookup(my_dict[\"sentences\"][sent_id][\"token_span_DOC\"], event_dict[\"start_char\"]) # updated on Oct 29, 2022, because of unknown error\n",
    "        my_dict[\"event_dict\"][event_id][\"_subword_id\"] = \\\n",
    "        id_lookup(my_dict[\"sentences\"][sent_id][\"_subword_span_DOC\"], event_dict[\"start_char\"]) + 1\n",
    "        # updated on Mar 20, 2021, plus 1 because of [CLS] or <s>\n",
    "        my_dict[\"event_dict\"][event_id][\"tense\"] = tense_finder(my_dict['sentences'][sent_id][\"tense_list\"], event_dict[\"start_char\"] - my_dict['sentences'][sent_id][\"sent_start_char\"])\n",
    "        # updated on Oct 24, 2022, because of change of tense identification service\n",
    "    return my_dict\n",
    "\n",
    "def convert_t5_input(text, tl):\n",
    "    #text = t5_TDD_dic['man-test']['APW19980308.0201'][0][0][19:]\n",
    "    pointer = text.find(\"<extra_id_\")\n",
    "    event_pos = []\n",
    "    event_pos_end = []\n",
    "    event_ids = []\n",
    "    e_dict = {}\n",
    "    while pointer != -1:\n",
    "        offset = 2\n",
    "        while text[pointer-offset].isalpha():\n",
    "            offset += 1\n",
    "        event_pos.append(pointer-offset+1)\n",
    "        event_pos_end.append(pointer-1)\n",
    "        offset = 1\n",
    "        while text[pointer+10+offset].isdigit():\n",
    "            offset += 1\n",
    "        event_id = text[pointer+10:pointer+10+offset]\n",
    "        event_ids.append(event_id)\n",
    "        assert text[pointer+10+offset] == '>'\n",
    "        offset += 1\n",
    "        if text[pointer+10+offset] == ' ':\n",
    "            offset += 1\n",
    "        text = text[0:pointer] + text[pointer+10+offset:]\n",
    "        pointer = text.find(\"<extra_id_\")\n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        e_dict[event_id] = i\n",
    "    timeline = []\n",
    "    abnormal = 0\n",
    "    for i in tl:\n",
    "        try:\n",
    "            timeline.append(e_dict[i.replace('<extra_id_', '').replace('>', '')])\n",
    "        except:\n",
    "            #print(\"abnormal\")\n",
    "            abnormal = 1\n",
    "    return text, event_pos, event_pos_end, event_ids, e_dict, timeline, abnormal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2b42d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tml_reader(dir_name, file_name, tokenizer):\n",
    "    my_dict = {}\n",
    "    my_dict[\"event_dict\"] = {}\n",
    "    my_dict[\"eiid_dict\"] = {}\n",
    "    my_dict[\"doc_id\"] = file_name.replace(\".tml\", \"\") \n",
    "    # e.g., file_name = \"ABC19980108.1830.0711.tml\"\n",
    "    # dir_name = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/TBAQ-cleaned/TimeBank/'\n",
    "    tree = ET.parse(dir_name + file_name)\n",
    "    root = tree.getroot()\n",
    "    MY_STRING = str(ET.tostring(root))\n",
    "    # ================================================\n",
    "    # Load the lines involving event information first\n",
    "    # ================================================\n",
    "    event_id_why = 0\n",
    "    for makeinstance in root.findall('MAKEINSTANCE'):\n",
    "        instance_str = str(ET.tostring(makeinstance)).split(\" \")\n",
    "        try:\n",
    "            assert instance_str[3].split(\"=\")[0] == \"eventID\"\n",
    "            assert instance_str[2].split(\"=\")[0] == \"eiid\"\n",
    "            eiid = int(instance_str[2].split(\"=\")[1].replace(\"\\\"\", \"\")[2:])\n",
    "            eID = instance_str[3].split(\"=\")[1].replace(\"\\\"\", \"\")\n",
    "        except:\n",
    "            for i in instance_str:\n",
    "                if i.split(\"=\")[0] == \"eventID\":\n",
    "                    eID = i.split(\"=\")[1].replace(\"\\\"\", \"\")\n",
    "                if i.split(\"=\")[0] == \"eiid\":\n",
    "                    eiid = int(i.split(\"=\")[1].replace(\"\\\"\", \"\")[2:])\n",
    "        # Not all document in the dataset contributes relation pairs in MATRES\n",
    "        # Not all events in a document constitute relation pairs in MATRES\n",
    "        \n",
    "        if my_dict[\"doc_id\"] in eiid_to_event_trigger.keys():\n",
    "            if eiid in eiid_to_event_trigger[my_dict[\"doc_id\"]].keys():\n",
    "                event_id_why += 1\n",
    "                my_dict[\"event_dict\"][eID] = {\"eiid\": eiid, \"mention\": eiid_to_event_trigger[my_dict[\"doc_id\"]][eiid], \"event_id_why\": event_id_why}\n",
    "                my_dict[\"eiid_dict\"][eiid] = {\"eID\": eID}\n",
    "        \n",
    "    # ==================================\n",
    "    #              Load Text\n",
    "    # ==================================\n",
    "    start = MY_STRING.find(\"<TEXT>\") + 6\n",
    "    end = MY_STRING.find(\"</TEXT>\")\n",
    "    MY_TEXT = MY_STRING[start:end]\n",
    "    while MY_TEXT[0] == \" \":\n",
    "        MY_TEXT = MY_TEXT[1:]\n",
    "    MY_TEXT = MY_TEXT.replace(\"\\\\n\", \" \")\n",
    "    MY_TEXT = MY_TEXT.replace(\"\\\\'\", \"\\'\")\n",
    "    MY_TEXT = MY_TEXT.replace(\"  \", \" \")\n",
    "    MY_TEXT = MY_TEXT.replace(\" ...\", \"...\")\n",
    "    \n",
    "    # ========================================================\n",
    "    #    Load position of events, in the meantime replacing \n",
    "    #    \"<EVENT eid=\"e1\" class=\"OCCURRENCE\">turning</EVENT>\"\n",
    "    #    with \"turning\"\n",
    "    # ========================================================\n",
    "    while MY_TEXT.find(\"<\") != -1:\n",
    "        start = MY_TEXT.find(\"<\")\n",
    "        end = MY_TEXT.find(\">\")\n",
    "        if MY_TEXT[start + 1] == \"E\":\n",
    "            event_description = MY_TEXT[start:end].split(\" \")\n",
    "            desp_idx = 1\n",
    "            if not event_description[desp_idx].split(\"=\")[0] == \"eid\":\n",
    "                desp_idx = 2\n",
    "            eID = (event_description[desp_idx].split(\"=\"))[1].replace(\"\\\"\", \"\")\n",
    "            MY_TEXT = MY_TEXT[:start] + MY_TEXT[(end + 1):]\n",
    "            if eID in my_dict[\"event_dict\"].keys():\n",
    "                my_dict[\"event_dict\"][eID][\"start_char\"] = start # loading position of events\n",
    "        else:\n",
    "            MY_TEXT = MY_TEXT[:start] + MY_TEXT[(end + 1):]\n",
    "    \n",
    "    # =====================================\n",
    "    # Enter the routine for text processing\n",
    "    # =====================================\n",
    "    \n",
    "    my_dict[\"doc_content\"] = MY_TEXT\n",
    "    my_dict[\"sentences\"] = []\n",
    "    my_dict[\"relation_dict\"] = {}\n",
    "    sent_tokenized_text = sent_tokenize(my_dict[\"doc_content\"])\n",
    "    #sent_tokenized_text = []\n",
    "    #tense_res = tense_getter(MY_TEXT)\n",
    "    #for sentence in tense_res['sentences']:\n",
    "    #    sent_tokenized_text.append(sentence[0])\n",
    "        \n",
    "    sent_span = tokenized_to_origin_span(my_dict[\"doc_content\"], sent_tokenized_text)\n",
    "    end_pos = [1]\n",
    "    for count_sent, sent in enumerate(sent_tokenized_text):\n",
    "        sent_dict = {}\n",
    "        sent_dict[\"sent_id\"] = count_sent\n",
    "        sent_dict[\"content\"] = sent\n",
    "        sent_dict[\"sent_start_char\"] = sent_span[count_sent][0]\n",
    "        sent_dict[\"sent_end_char\"] = sent_span[count_sent][1]\n",
    "        sent_dict[\"tense_list\"] = tense_getter(sent)\n",
    "        \n",
    "        spacy_token = nlp(sent_dict[\"content\"])\n",
    "        sent_dict[\"tokens\"] = []\n",
    "        sent_dict[\"pos\"] = []\n",
    "        # spaCy-tokenized tokens & Part-Of-Speech Tagging\n",
    "        for token in spacy_token:\n",
    "            sent_dict[\"tokens\"].append(token.text)\n",
    "            sent_dict[\"pos\"].append(token.pos_)\n",
    "        sent_dict[\"token_span_SENT\"] = tokenized_to_origin_span(sent, sent_dict[\"tokens\"])\n",
    "        sent_dict[\"token_span_DOC\"] = span_SENT_to_DOC(sent_dict[\"token_span_SENT\"], sent_dict[\"sent_start_char\"])\n",
    "\n",
    "        # huggingface tokenizer\n",
    "        sent_dict[\"_subword_to_ID\"], sent_dict[\"_subwords\"], \\\n",
    "        sent_dict[\"_subword_span_SENT\"], sent_dict[\"_subword_map\"] = \\\n",
    "        transformers_list(sent_dict[\"content\"], tokenizer, sent_dict[\"tokens\"], sent_dict[\"token_span_SENT\"])\n",
    "            \n",
    "        if count_sent == 0:\n",
    "            end_pos.append(len(sent_dict[\"_subword_to_ID\"]))\n",
    "        else:\n",
    "            end_pos.append(end_pos[-1] + len(sent_dict[\"_subword_to_ID\"]) - 1)\n",
    "            \n",
    "        sent_dict[\"_subword_span_DOC\"] = \\\n",
    "        span_SENT_to_DOC(sent_dict[\"_subword_span_SENT\"], sent_dict[\"sent_start_char\"])\n",
    "        \n",
    "        sent_dict[\"_subword_pos\"] = []\n",
    "        for token_id in sent_dict[\"_subword_map\"]:\n",
    "            if token_id == -1 or token_id is None:\n",
    "                sent_dict[\"_subword_pos\"].append(\"None\")\n",
    "            else:\n",
    "                sent_dict[\"_subword_pos\"].append(sent_dict[\"pos\"][token_id])\n",
    "        \n",
    "        my_dict[\"sentences\"].append(sent_dict)\n",
    "        count_sent += 1\n",
    "        \n",
    "    my_dict['end_pos'] = end_pos\n",
    "    # Add sent_id as an attribute of event\n",
    "    for event_id, event_dict in my_dict[\"event_dict\"].items():\n",
    "        my_dict[\"event_dict\"][event_id][\"sent_id\"] = sent_id = \\\n",
    "        sent_id_lookup(my_dict, event_dict[\"start_char\"])\n",
    "        my_dict[\"event_dict\"][event_id][\"token_id\"] = \\\n",
    "        id_lookup(my_dict[\"sentences\"][sent_id][\"token_span_DOC\"], event_dict[\"start_char\"])\n",
    "        my_dict[\"event_dict\"][event_id][\"_subword_id\"] = \\\n",
    "        id_lookup(my_dict[\"sentences\"][sent_id][\"_subword_span_DOC\"], event_dict[\"start_char\"]) + 1 \n",
    "        # updated on Mar 20, 2021\n",
    "        #my_dict[\"event_dict\"][event_id][\"tense\"] = tense_finder(tense_res['sentences'][sent_id][1], event_dict[\"start_char\"] - my_dict['sentences'][sent_id][\"sent_start_char\"])\n",
    "        # updated on Feb 21, 2021\n",
    "        my_dict[\"event_dict\"][event_id][\"tense\"] = tense_finder(my_dict['sentences'][sent_id][\"tense_list\"], event_dict[\"start_char\"] - my_dict['sentences'][sent_id][\"sent_start_char\"])\n",
    "        # updated on Oct 24, 2022, because of change of tense identification service\n",
    "        \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ce6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath_TB = './MATRES_old/TBAQ-cleaned/TimeBank/' # after correction\n",
    "onlyfiles_TB = [f for f in listdir(mypath_TB) if isfile(join(mypath_TB, f))]\n",
    "mypath_AQ = './MATRES_old/TBAQ-cleaned/AQUAINT/' \n",
    "onlyfiles_AQ = [f for f in listdir(mypath_AQ) if isfile(join(mypath_AQ, f))]\n",
    "mypath_PL = './MATRES_old/te3-platinum/'\n",
    "onlyfiles_PL = [f for f in listdir(mypath_PL) if isfile(join(mypath_PL, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "740e7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#        MATRES: read relation file\n",
    "# ========================================\n",
    "# MATRES has separate text files and relation files\n",
    "# We first read relation files\n",
    "\n",
    "\n",
    "MATRES_timebank = './MATRES_old/timebank.txt'\n",
    "MATRES_aquaint = './MATRES_old/aquaint.txt'\n",
    "MATRES_platinum = './MATRES_old/platinum.txt'\n",
    "temp_label_map = {\"BEFORE\": 0, \"AFTER\": 1, \"EQUAL\": 2, \"VAGUE\": 3}\n",
    "eiid_to_event_trigger = {}\n",
    "eiid_pair_to_label = {}   \n",
    "\n",
    "# =========================\n",
    "#       MATRES Reader\n",
    "# =========================\n",
    "def MATRES_READER(matres_file, eiid_to_event_trigger, eiid_pair_to_label):\n",
    "    with open(matres_file, \"r\") as f_matres:\n",
    "        content = f_matres.read().split(\"\\n\")\n",
    "#         assert len(content[-1].split(\"\\t\")) == 1\n",
    "#         content = content[:-1]\n",
    "        for rel in content:\n",
    "            rel = rel.split(\"\\t\")\n",
    "            fname = rel[0]\n",
    "            trigger1 = rel[1]\n",
    "            trigger2 = rel[2]\n",
    "            eiid1 = int(rel[3])\n",
    "            eiid2 = int(rel[4])\n",
    "            tempRel = temp_label_map[rel[5]]\n",
    "\n",
    "            if fname not in eiid_to_event_trigger:\n",
    "                eiid_to_event_trigger[fname] = {}\n",
    "                eiid_pair_to_label[fname] = {}\n",
    "            eiid_pair_to_label[fname][(eiid1, eiid2)] = tempRel\n",
    "            if eiid1 not in eiid_to_event_trigger[fname].keys():\n",
    "                eiid_to_event_trigger[fname][eiid1] = trigger1\n",
    "            if eiid2 not in eiid_to_event_trigger[fname].keys():\n",
    "                eiid_to_event_trigger[fname][eiid2] = trigger2\n",
    "\n",
    "MATRES_READER(MATRES_timebank, eiid_to_event_trigger, eiid_pair_to_label)\n",
    "MATRES_READER(MATRES_aquaint, eiid_to_event_trigger, eiid_pair_to_label)\n",
    "MATRES_READER(MATRES_platinum, eiid_to_event_trigger, eiid_pair_to_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4ea5a",
   "metadata": {},
   "source": [
    "# demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def803e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 03/10/2023 03:16:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime \n",
    "import random\n",
    "from matres_reader_with_tense import *\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import DataLoader\n",
    "from util import *\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from model import transformers_mlp_cons\n",
    "from exp import *\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from synonyms import *\n",
    "import pickle\n",
    "from timeline_construct import *\n",
    "from ts import func, ModelWithTemperature\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# datetime object containing current date and time\n",
    "now = datetime.datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "\n",
    "#label_dict={\"SuperSub\": 0, \"SubSuper\": 1, \"Coref\": 2, \"NoRel\": 3}\n",
    "num_dict = {0: \"before\", 1: \"after\", 2: \"equal\", 3: \"vague\"}\n",
    "#def label_to_num(label):\n",
    "#    return label_dict[label]\n",
    "def num_to_label(num):\n",
    "    return num_dict[num]\n",
    "\n",
    "mask_in_input_ids = 0 # note that [MASK] is actually learned through pre-training\n",
    "mask_in_input_mask = 0 # when input is masked through attention, it would be replaced with [PAD]\n",
    "acronym = 0 # using acronym for tense (e.g., pastsimp): 1; else (e.g., past simple): 0\n",
    "t_marker = 1\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "    \n",
    "def collate_fn(batch):\n",
    "    max_len = max([len(f['input_ids']) for f in batch])\n",
    "    input_ids = [f['input_ids'] + [0] * (max_len - len(f['input_ids'])) for f in batch]\n",
    "    if mask_in_input_ids:\n",
    "        input_ids_new = []\n",
    "        for f_id, f in enumerate(input_ids):\n",
    "            for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "                end = batch[f_id]['event_pos_end'][event_id]\n",
    "                for token_id in range(start, end): # needs verification\n",
    "                    f[token_id] = 67\n",
    "            input_ids_new.append(f)\n",
    "        input_ids = input_ids_new\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    input_mask = [[1.0] * len(f['input_ids']) + [0.0] * (max_len - len(f['input_ids'])) for f in batch]\n",
    "    if mask_in_input_mask:\n",
    "        input_mask_new = []\n",
    "        for f_id, f in enumerate(input_mask):\n",
    "            for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "                end = batch[f_id]['event_pos_end'][event_id]\n",
    "                for token_id in range(start, end): # needs verification\n",
    "                    f[token_id] = 0.0\n",
    "            input_mask_new.append(f)\n",
    "        input_mask = input_mask_new\n",
    "    # Updated on May 17, 2022    \n",
    "    input_mask_eo = [[0.0] * max_len for f in batch]\n",
    "    for f_id, f in enumerate(input_mask_eo):\n",
    "        for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "            end = batch[f_id]['event_pos_end'][event_id]\n",
    "            for token_id in range(start, end): # needs verification\n",
    "                f[token_id] = 1.0\n",
    "    # Updated on Jun 14, 2022\n",
    "    input_mask_xbar = [[0.0] * max_len for f in batch]\n",
    "    input_mask_xbar = torch.tensor(input_mask_xbar, dtype=torch.float)\n",
    "    input_mask_eo = torch.tensor(input_mask_eo, dtype=torch.float)\n",
    "    input_mask = torch.tensor(input_mask, dtype=torch.float)\n",
    "    event_pos = [f['event_pos'] for f in batch]\n",
    "    event_pos_end = [f['event_pos_end'] for f in batch]\n",
    "    event_pair = [f['event_pair'] for f in batch]\n",
    "    labels = [f['labels'] for f in batch]\n",
    "    output = (input_ids, input_mask, event_pos, event_pos_end, event_pair, labels, input_mask_eo, input_mask_xbar)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ce1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = 'micro'\n",
    "params = {'transformers_model': 'google/bigbird-roberta-large',\n",
    "          'dataset': 'MATRES',   # 'HiEve', 'IC', 'MATRES' \n",
    "          'testdata': 'None', # MATRES / MATRES_nd / TDD / PRED / None; None means training mode\n",
    "          'block_size': 64,\n",
    "          'add_loss': 0, \n",
    "          'batch_size': 1,    # 6 works on 48G gpu\n",
    "          'epochs': 40,\n",
    "          'learning_rate': 5e-6,    # subject to change\n",
    "          'seed': 0,\n",
    "          'gpu_id': '11453',    # subject to change\n",
    "          'debug': 0,\n",
    "          'rst_file_name': '0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst',    # subject to change\n",
    "          'mask_in_input_ids': mask_in_input_ids,\n",
    "          'mask_in_input_mask': mask_in_input_mask,\n",
    "          'marker': 'abc', \n",
    "          'tense_acron': 0, # 1 (acronym of tense) or 0 (original tense)\n",
    "          't_marker': 1, # 2 (trigger enclosed by special tokens) or 1 (tense enclosed by **)\n",
    "          'td': 1, # 0 (no tense detection) or 1 (tense detection, add tense info)\n",
    "          'dpn': 1, # 1 if use DPN; else 0\n",
    "          'lambda_1': -10, # lower bound * 10\n",
    "          'lambda_2': 11, # upper bound * 10\n",
    "          'f1_metric': f1_metric, \n",
    "         }\n",
    "# $acr $tmarker $td $dpn $mask $lambda_1 $lambda_2\n",
    "\n",
    "# FOR 48GBgpu\n",
    "if params['testdata'] in ['MATRES', 'MATRES_nd']:\n",
    "    #params['batch_size'] = 400\n",
    "    params['batch_size'] = 1\n",
    "if params['testdata'] in ['TDD']:\n",
    "    params['batch_size'] = 100\n",
    "    \n",
    "if params['testdata'] == 'MATRES_nd':\n",
    "    params['nd'] = True\n",
    "else:\n",
    "    params['nd'] = False\n",
    "    \n",
    "###########\n",
    "# NO MASK #\n",
    "###########\n",
    "\n",
    "if params['rst_file_name'] == '0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst':\n",
    "    slurm_id = '11453'\n",
    "#params['rst_file_name'] = '0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst' \n",
    "#slurm_id = '11453'\n",
    "# python main_pair.py 0615_11453.rst 5e-6 400 11453 0 MATRES abc 0 1 1 1 0 -10 11\n",
    "# python main_pair.py 0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst 5e-6 400 11453 0 MATRES abc 0 1 1 1 0 -10 11\n",
    "\n",
    "if params['transformers_model'][-5:] == \"large\":\n",
    "    params['emb_size'] = 1024\n",
    "elif params['transformers_model'][-4:] == \"base\":\n",
    "    params['emb_size'] = 768\n",
    "else:\n",
    "    print(\"emb_size is neither 1024 nor 768? ...\")\n",
    "    \n",
    "set_seed(params['seed'])\n",
    "rst_file_name = params['rst_file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c408d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MATRES dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-large were not used when initializing BigBirdModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing \" + params['dataset'] + \" dataset...\")\n",
    "t0 = time.time()\n",
    "if params['dataset'] == \"IC\":\n",
    "    dir_name = \"./IC/IC_Processed/\"\n",
    "    #max_sent_len = 193\n",
    "elif params['dataset'] == \"HiEve\":\n",
    "    dir_name = \"./hievents_v2/processed/\"\n",
    "    #max_sent_len = 155\n",
    "elif params['dataset'] == \"MATRES\":\n",
    "    dir_name = \"\"\n",
    "else:\n",
    "    print(\"Not supporting this dataset yet!\")\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(params['transformers_model'])   \n",
    "if acronym:\n",
    "    special_tokens_dict = {'additional_special_tokens': \n",
    "                           [' [futuperfsimp]',' [futucont]',' [futuperfcont]',' [futusimp]', ' [pastcont]', ' [pastperfcont]', ' [pastperfsimp]', ' [pastsimp]', ' [prescont]', ' [presperfcont]', ' [presperfsimp]', ' [pressimp]', ' [futuperfsimppass]',' [futucontpass]',' [futuperfcontpass]',' [futusimppass]', ' [pastcontpass]', ' [pastperfcontpass]', ' [pastperfsimppass]', ' [pastsimppass]', ' [prescontpass]', ' [presperfcontpass]', ' [presperfsimppass]', ' [pressimppass]', ' [none]'\n",
    "                           ]}\n",
    "    spec_toke_list = []\n",
    "    for t in special_tokens_dict['additional_special_tokens']:\n",
    "        spec_toke_list.append(\" [/\" + t[2:])\n",
    "    special_tokens_dict['additional_special_tokens'] += spec_toke_list\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model = AutoModel.from_pretrained(params['transformers_model'])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(params['transformers_model'])\n",
    "params['model'] = model\n",
    "debug = params['debug']\n",
    "if debug:\n",
    "    params['epochs'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47bbb470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28ba38cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                                                                      | 2/274 [00:00<00:51,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first event of the pair: predicted\n",
      "second event of the pair: tried\n",
      "TokenIDs: [CLS] For his part, Fidel Castro is the ultimate political survivor.[SEP] People have @ * Present Perfect Simple * predicted @ his demise so many times, and the US has # ~ Present Perfect Simple ~ tried # to hasten it on several occasions.[SEP] Time and again, he endures.[SEP]\n",
      "first event of the pair: predicted\n",
      "second event of the pair: hasten\n",
      "TokenIDs: [CLS] For his part, Fidel Castro is the ultimate political survivor.[SEP] People have @ * Present Perfect Simple * predicted @ his demise so many times, and the US has tried to # ~ None ~ hasten # it on several occasions.[SEP] Time and again, he endures.[SEP]\n",
      "first event of the pair: tried\n",
      "second event of the pair: hasten\n",
      "TokenIDs: [CLS] For his part, Fidel Castro is the ultimate political survivor.[SEP] People have predicted his demise so many times, and the US has @ * Present Perfect Simple * tried @ to # ~ None ~ hasten # it on several occasions.[SEP] Time and again, he endures.[SEP]\n",
      "first event of the pair: predicted\n",
      "second event of the pair: endures\n",
      "TokenIDs: [CLS] For his part, Fidel Castro is the ultimate political survivor.[SEP] People have @ * Present Perfect Simple * predicted @ his demise so many times, and the US has tried to hasten it on several occasions.[SEP] Time and again, he # ~ Present Simple ~ endures #.[SEP] He has outlasted and sometimes outsmarted eight American presidents.[SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 274/274 [01:01<00:00,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRES Preprocessing took 0:01:01\n",
      "Temporal Relation Stats: {0: 6874, 1: 4570, 2: 470, 3: 1644}\n",
      "Total num of pairs: 13558\n",
      "Max length of context: 245\n",
      "Avg num of sentences that context contains: 3.562546098244579\n",
      "Context length stats(unit: sentence):  {3: 4911, 4: 7981, 2: 554, 5: 106, 1: 6}\n",
      "MATRES train valid test pair num: 6336 6404 818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = -1\n",
    "features_train = []\n",
    "features_valid = []\n",
    "features_test = []\n",
    "t0 = time.time()\n",
    "relation_stats = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "t_marker = params['t_marker']\n",
    "# 2: will [futusimp] begin [/futusimp]\n",
    "# 1: will @ * Future Simple * begin @ \n",
    "\n",
    "max_len = 0\n",
    "sent_num = 0\n",
    "pair_num = 0\n",
    "test_labels = []\n",
    "context_len = {}\n",
    "timeline_input = []\n",
    "for fname in tqdm.tqdm(eiid_pair_to_label.keys()):\n",
    "    file_name = fname + \".tml\"\n",
    "    if file_name in onlyfiles_TB:\n",
    "        dir_name = mypath_TB\n",
    "    elif file_name in onlyfiles_AQ:\n",
    "        dir_name = mypath_AQ\n",
    "    elif file_name in onlyfiles_PL:\n",
    "        dir_name = mypath_PL\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    my_dict = tml_reader(dir_name, file_name, tokenizer) \n",
    "    \n",
    "    for (eiid1, eiid2) in eiid_pair_to_label[fname].keys():\n",
    "        pair_num += 1\n",
    "        event_pos = []\n",
    "        event_pos_end = []\n",
    "        relations = []\n",
    "        TokenIDs = [65]\n",
    "        if eiid1 == 0:\n",
    "            print('error', fname, eiid1)\n",
    "            continue\n",
    "        x = my_dict[\"eiid_dict\"][eiid1][\"eID\"] # eID\n",
    "        y = my_dict[\"eiid_dict\"][eiid2][\"eID\"]\n",
    "        x_sent_id = my_dict[\"event_dict\"][x][\"sent_id\"]\n",
    "        y_sent_id = my_dict[\"event_dict\"][y][\"sent_id\"]\n",
    "        reverse = False\n",
    "        if x_sent_id > y_sent_id:\n",
    "            reverse = True\n",
    "            x = my_dict[\"eiid_dict\"][eiid2][\"eID\"]\n",
    "            y = my_dict[\"eiid_dict\"][eiid1][\"eID\"]\n",
    "            x_sent_id = my_dict[\"event_dict\"][x][\"sent_id\"]\n",
    "            y_sent_id = my_dict[\"event_dict\"][y][\"sent_id\"]\n",
    "        elif x_sent_id == y_sent_id:\n",
    "            x_position = my_dict[\"event_dict\"][x][\"_subword_id\"]\n",
    "            y_position = my_dict[\"event_dict\"][y][\"_subword_id\"]\n",
    "            if x_position > y_position:\n",
    "                reverse = True\n",
    "                x = my_dict[\"eiid_dict\"][eiid2][\"eID\"]\n",
    "                y = my_dict[\"eiid_dict\"][eiid1][\"eID\"]\n",
    "        x_sent = my_dict[\"sentences\"][x_sent_id][\"_subword_to_ID\"]\n",
    "        y_sent = my_dict[\"sentences\"][y_sent_id][\"_subword_to_ID\"]\n",
    "        # This guarantees that trigger x is always before trigger y in narrative order\n",
    "\n",
    "        context_start_sent_id = max(x_sent_id-1, 0)\n",
    "        context_end_sent_id = min(y_sent_id+2, len(my_dict[\"sentences\"]))\n",
    "        c_len = context_end_sent_id - context_start_sent_id\n",
    "        if c_len in context_len.keys():\n",
    "            context_len[c_len] += 1\n",
    "        else:\n",
    "            context_len[c_len] = 1\n",
    "        sent_num += c_len\n",
    "        \n",
    "        if params['td'] == 1:\n",
    "            x_sent, offset_x, new_start_x, new_end_x = add_tense_info(x_sent, my_dict[\"event_dict\"][x]['tense'], my_dict['event_dict'][x]['_subword_id'], my_dict[\"event_dict\"][x]['mention'], 2589, 1736)\n",
    "        else:\n",
    "            x_sent, offset_x, new_start_x, new_end_x = x_sent, 0, my_dict['event_dict'][x]['_subword_id'], my_dict['event_dict'][x]['_subword_id'] + len(tokenizer.encode(my_dict[\"event_dict\"][x]['mention'])) - 2\n",
    "            \n",
    "        if x_sent_id != y_sent_id:\n",
    "            if params['td'] == 1:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = add_tense_info(y_sent, my_dict[\"event_dict\"][y]['tense'], my_dict['event_dict'][y]['_subword_id'], my_dict[\"event_dict\"][y]['mention'], 1404, 5400)\n",
    "            else:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = y_sent, 0, my_dict['event_dict'][y]['_subword_id'], my_dict['event_dict'][y]['_subword_id'] + len(tokenizer.encode(my_dict[\"event_dict\"][y]['mention'])) - 2\n",
    "            for sid in range(context_start_sent_id, context_end_sent_id):\n",
    "                if sid == x_sent_id:\n",
    "                    event_pos.append(new_start_x + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_x + len(TokenIDs) - 1)\n",
    "                    TokenIDs += x_sent[1:]\n",
    "                elif sid == y_sent_id:\n",
    "                    event_pos.append(new_start_y + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_y + len(TokenIDs) - 1)\n",
    "                    TokenIDs += y_sent[1:]\n",
    "                else:\n",
    "                    TokenIDs += my_dict[\"sentences\"][sid][\"_subword_to_ID\"][1:]\n",
    "        else:\n",
    "            if params['td'] == 1:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = add_tense_info(x_sent, my_dict[\"event_dict\"][y]['tense'], my_dict['event_dict'][y]['_subword_id'] + offset_x, my_dict[\"event_dict\"][y]['mention'], 1404, 5400)\n",
    "            else:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = y_sent, 0, my_dict['event_dict'][y]['_subword_id'], my_dict['event_dict'][y]['_subword_id'] + len(tokenizer.encode(my_dict[\"event_dict\"][y]['mention'])) - 2\n",
    "            for sid in range(context_start_sent_id, context_end_sent_id):\n",
    "                if sid == y_sent_id:\n",
    "                    event_pos.append(new_start_x + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_x + len(TokenIDs) - 1)\n",
    "                    event_pos.append(new_start_y + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_y + len(TokenIDs) - 1)\n",
    "                    TokenIDs += y_sent[1:]\n",
    "                else:\n",
    "                    TokenIDs += my_dict[\"sentences\"][sid][\"_subword_to_ID\"][1:]\n",
    "                    \n",
    "        if reverse:\n",
    "            event_pos = reverse_num(event_pos)\n",
    "            event_pos_end = reverse_num(event_pos_end)\n",
    "            \n",
    "        xy = eiid_pair_to_label[fname][(eiid1, eiid2)]\n",
    "        \n",
    "        relations.append(xy)\n",
    "        relation_stats[xy] += 1\n",
    "        if len(TokenIDs) > max_len:\n",
    "            max_len = len(TokenIDs)\n",
    "        \n",
    "        if debug or pair_num < 5:\n",
    "            print(\"first event of the pair:\", tokenizer.decode(TokenIDs[event_pos[0]:event_pos_end[0]]))\n",
    "            print(\"second event of the pair:\", tokenizer.decode(TokenIDs[event_pos[1]:event_pos_end[1]]))\n",
    "            print(\"TokenIDs:\", tokenizer.decode(TokenIDs))\n",
    "        \n",
    "        if params['nd']:\n",
    "            syn_0 = replace_with_syn(tokenizer.decode(TokenIDs[event_pos[0]:event_pos_end[0]]))\n",
    "            syn_1 = replace_with_syn(tokenizer.decode(TokenIDs[event_pos[1]:event_pos_end[1]]))\n",
    "            if len(syn_0) > 0:\n",
    "                TokenIDs = TokenIDs[0:event_pos[0]] + tokenizer.encode(syn_0[0])[1:-1] + TokenIDs[event_pos_end[0]:]\n",
    "                prev = event_pos_end[0]\n",
    "                event_pos_end[0] = event_pos[0] + len(tokenizer.encode(syn_0[0])[1:-1])\n",
    "                if prev != event_pos_end[0]:\n",
    "                    offset = event_pos_end[0] - prev\n",
    "                    event_pos[1] += offset\n",
    "                    event_pos_end[1] += offset\n",
    "            if len(syn_1) > 0:\n",
    "                TokenIDs = TokenIDs[0:event_pos[1]] + tokenizer.encode(syn_1[0])[1:-1] + TokenIDs[event_pos_end[1]:]\n",
    "                prev = event_pos_end[1]\n",
    "                event_pos_end[1] = event_pos[1] + len(tokenizer.encode(syn_1[0])[1:-1])\n",
    "            #assert 1 == 0\n",
    "        feature = {'input_ids': TokenIDs,\n",
    "                   'event_pos': event_pos,\n",
    "                   'event_pos_end': event_pos_end,\n",
    "                   'event_pair': [[1, 2]],\n",
    "                   'labels': relations,\n",
    "                  }\n",
    "        if file_name in onlyfiles_TB:\n",
    "            features_train.append(feature)\n",
    "        elif file_name in onlyfiles_AQ:\n",
    "            features_valid.append(feature)\n",
    "        elif file_name in onlyfiles_PL:\n",
    "            features_test.append(feature)\n",
    "            test_labels.append(xy)\n",
    "            timeline_input.append([fname, x, y, xy])\n",
    "    if debug:\n",
    "        break\n",
    "        \n",
    "elapsed = format_time(time.time() - t0)\n",
    "print(\"MATRES Preprocessing took {:}\".format(elapsed)) \n",
    "print(\"Temporal Relation Stats:\", relation_stats)\n",
    "print(\"Total num of pairs:\", pair_num)\n",
    "print(\"Max length of context:\", max_len)\n",
    "print(\"Avg num of sentences that context contains:\", sent_num/pair_num)\n",
    "print(\"Context length stats(unit: sentence): \", context_len)\n",
    "print(\"MATRES train valid test pair num:\", len(features_train), len(features_valid), len(features_test))\n",
    "#with open(\"MATRES_test_timeline_input.json\", 'w') as f:\n",
    "#    json.dump(timeline_input, f)\n",
    "#    assert 0 == 1\n",
    "    \n",
    "#output_file = open('test_labels.txt', 'w')\n",
    "#for label in test_labels:\n",
    "#    output_file.write(str(label) + '\\n')\n",
    "#output_file.close()\n",
    "#if debug:\n",
    "#    assert 0 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc9183c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['nd'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d45d75",
   "metadata": {},
   "source": [
    "# unit test text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90d277dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./MATRES_old/te3-platinum/',\n",
       " 'CNN_20130321_821.tml',\n",
       " BigBirdTokenizerFast(name_or_path='google/bigbird-roberta-large', vocab_size=50358, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dir_name, file_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de95522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "my_dict[\"event_dict\"] = {}\n",
    "my_dict[\"eiid_dict\"] = {}\n",
    "my_dict[\"doc_id\"] = file_name.replace(\".tml\", \"\") \n",
    "# e.g., file_name = \"ABC19980108.1830.0711.tml\"\n",
    "# dir_name = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/TBAQ-cleaned/TimeBank/'\n",
    "tree = ET.parse(dir_name + file_name)\n",
    "root = tree.getroot()\n",
    "MY_STRING = str(ET.tostring(root))\n",
    "# ================================================\n",
    "# Load the lines involving event information first\n",
    "# ================================================\n",
    "event_id_why = 0\n",
    "for makeinstance in root.findall('MAKEINSTANCE'):\n",
    "    instance_str = str(ET.tostring(makeinstance)).split(\" \")\n",
    "    try:\n",
    "        assert instance_str[3].split(\"=\")[0] == \"eventID\"\n",
    "        assert instance_str[2].split(\"=\")[0] == \"eiid\"\n",
    "        eiid = int(instance_str[2].split(\"=\")[1].replace(\"\\\"\", \"\")[2:])\n",
    "        eID = instance_str[3].split(\"=\")[1].replace(\"\\\"\", \"\")\n",
    "    except:\n",
    "        for i in instance_str:\n",
    "            if i.split(\"=\")[0] == \"eventID\":\n",
    "                eID = i.split(\"=\")[1].replace(\"\\\"\", \"\")\n",
    "            if i.split(\"=\")[0] == \"eiid\":\n",
    "                eiid = int(i.split(\"=\")[1].replace(\"\\\"\", \"\")[2:])\n",
    "    # Not all document in the dataset contributes relation pairs in MATRES\n",
    "    # Not all events in a document constitute relation pairs in MATRES\n",
    "\n",
    "    if my_dict[\"doc_id\"] in eiid_to_event_trigger.keys():\n",
    "        if eiid in eiid_to_event_trigger[my_dict[\"doc_id\"]].keys():\n",
    "            event_id_why += 1\n",
    "            my_dict[\"event_dict\"][eID] = {\"eiid\": eiid, \"mention\": eiid_to_event_trigger[my_dict[\"doc_id\"]][eiid], \"event_id_why\": event_id_why}\n",
    "            my_dict[\"eiid_dict\"][eiid] = {\"eID\": eID}\n",
    "\n",
    "# ==================================\n",
    "#              Load Text\n",
    "# ==================================\n",
    "start = MY_STRING.find(\"<TEXT>\") + 6\n",
    "end = MY_STRING.find(\"</TEXT>\")\n",
    "MY_TEXT = MY_STRING[start:end]\n",
    "while MY_TEXT[0] == \" \":\n",
    "    MY_TEXT = MY_TEXT[1:]\n",
    "MY_TEXT = MY_TEXT.replace(\"\\\\n\", \" \")\n",
    "MY_TEXT = MY_TEXT.replace(\"\\\\'\", \"\\'\")\n",
    "MY_TEXT = MY_TEXT.replace(\"  \", \" \")\n",
    "MY_TEXT = MY_TEXT.replace(\" ...\", \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e37c90c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<EVENT', 'class=\"OCCURRENCE\"', 'eid=\"e1\"']\n",
      "['<EVENT', 'class=\"STATE\"', 'eid=\"e4\"']\n",
      "['<EVENT', 'class=\"OCCURRENCE\"', 'eid=\"e5\"']\n",
      "['<EVENT', 'class=\"OCCURRENCE\"', 'eid=\"e8\"']\n",
      "['<EVENT', 'eid=\"e1000\"', 'class=\"I_ACTION\"']\n",
      "['<EVENT', 'class=\"I_ACTION\"', 'eid=\"e1002\"']\n",
      "['<EVENT', 'class=\"OCCURRENCE\"', 'eid=\"e1003\"']\n",
      "['<EVENT', 'class=\"I_ACTION\"', 'eid=\"e20\"']\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "#    Load position of events, in the meantime replacing \n",
    "#    \"<EVENT eid=\"e1\" class=\"OCCURRENCE\">turning</EVENT>\"\n",
    "#    with \"turning\"\n",
    "# ========================================================\n",
    "while MY_TEXT.find(\"<\") != -1:\n",
    "    start = MY_TEXT.find(\"<\")\n",
    "    end = MY_TEXT.find(\">\")\n",
    "    if MY_TEXT[start + 1] == \"E\":\n",
    "        event_description = MY_TEXT[start:end].split(\" \")\n",
    "        print(event_description)\n",
    "        eID = (event_description[1].split(\"=\"))[1].replace(\"\\\"\", \"\")\n",
    "        MY_TEXT = MY_TEXT[:start] + MY_TEXT[(end + 1):]\n",
    "        if eID in my_dict[\"event_dict\"].keys():\n",
    "            my_dict[\"event_dict\"][eID][\"start_char\"] = start # loading position of events\n",
    "    else:\n",
    "        MY_TEXT = MY_TEXT[:start] + MY_TEXT[(end + 1):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54500fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<EVENT', 'class=\"REPORTING\"', 'eid=\"e55\"']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28195d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ae578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Enter the routine for text processing\n",
    "# =====================================\n",
    "\n",
    "my_dict[\"doc_content\"] = MY_TEXT\n",
    "my_dict[\"sentences\"] = []\n",
    "my_dict[\"relation_dict\"] = {}\n",
    "sent_tokenized_text = sent_tokenize(my_dict[\"doc_content\"])\n",
    "#sent_tokenized_text = []\n",
    "#tense_res = tense_getter(MY_TEXT)\n",
    "#for sentence in tense_res['sentences']:\n",
    "#    sent_tokenized_text.append(sentence[0])\n",
    "\n",
    "sent_span = tokenized_to_origin_span(my_dict[\"doc_content\"], sent_tokenized_text)\n",
    "end_pos = [1]\n",
    "for count_sent, sent in enumerate(sent_tokenized_text):\n",
    "    sent_dict = {}\n",
    "    sent_dict[\"sent_id\"] = count_sent\n",
    "    sent_dict[\"content\"] = sent\n",
    "    sent_dict[\"sent_start_char\"] = sent_span[count_sent][0]\n",
    "    sent_dict[\"sent_end_char\"] = sent_span[count_sent][1]\n",
    "    sent_dict[\"tense_list\"] = tense_getter(sent)\n",
    "\n",
    "    spacy_token = nlp(sent_dict[\"content\"])\n",
    "    sent_dict[\"tokens\"] = []\n",
    "    sent_dict[\"pos\"] = []\n",
    "    # spaCy-tokenized tokens & Part-Of-Speech Tagging\n",
    "    for token in spacy_token:\n",
    "        sent_dict[\"tokens\"].append(token.text)\n",
    "        sent_dict[\"pos\"].append(token.pos_)\n",
    "    sent_dict[\"token_span_SENT\"] = tokenized_to_origin_span(sent, sent_dict[\"tokens\"])\n",
    "    sent_dict[\"token_span_DOC\"] = span_SENT_to_DOC(sent_dict[\"token_span_SENT\"], sent_dict[\"sent_start_char\"])\n",
    "\n",
    "    # huggingface tokenizer\n",
    "    sent_dict[\"_subword_to_ID\"], sent_dict[\"_subwords\"], \\\n",
    "    sent_dict[\"_subword_span_SENT\"], sent_dict[\"_subword_map\"] = \\\n",
    "    transformers_list(sent_dict[\"content\"], tokenizer, sent_dict[\"tokens\"], sent_dict[\"token_span_SENT\"])\n",
    "\n",
    "    if count_sent == 0:\n",
    "        end_pos.append(len(sent_dict[\"_subword_to_ID\"]))\n",
    "    else:\n",
    "        end_pos.append(end_pos[-1] + len(sent_dict[\"_subword_to_ID\"]) - 1)\n",
    "\n",
    "    sent_dict[\"_subword_span_DOC\"] = \\\n",
    "    span_SENT_to_DOC(sent_dict[\"_subword_span_SENT\"], sent_dict[\"sent_start_char\"])\n",
    "\n",
    "    sent_dict[\"_subword_pos\"] = []\n",
    "    for token_id in sent_dict[\"_subword_map\"]:\n",
    "        if token_id == -1 or token_id is None:\n",
    "            sent_dict[\"_subword_pos\"].append(\"None\")\n",
    "        else:\n",
    "            sent_dict[\"_subword_pos\"].append(sent_dict[\"pos\"][token_id])\n",
    "\n",
    "    my_dict[\"sentences\"].append(sent_dict)\n",
    "    count_sent += 1\n",
    "\n",
    "my_dict['end_pos'] = end_pos\n",
    "# Add sent_id as an attribute of event\n",
    "for event_id, event_dict in my_dict[\"event_dict\"].items():\n",
    "    my_dict[\"event_dict\"][event_id][\"sent_id\"] = sent_id = \\\n",
    "    sent_id_lookup(my_dict, event_dict[\"start_char\"])\n",
    "    my_dict[\"event_dict\"][event_id][\"token_id\"] = \\\n",
    "    id_lookup(my_dict[\"sentences\"][sent_id][\"token_span_DOC\"], event_dict[\"start_char\"])\n",
    "    my_dict[\"event_dict\"][event_id][\"_subword_id\"] = \\\n",
    "    id_lookup(my_dict[\"sentences\"][sent_id][\"_subword_span_DOC\"], event_dict[\"start_char\"]) + 1 \n",
    "    # updated on Mar 20, 2021\n",
    "    #my_dict[\"event_dict\"][event_id][\"tense\"] = tense_finder(tense_res['sentences'][sent_id][1], event_dict[\"start_char\"] - my_dict['sentences'][sent_id][\"sent_start_char\"])\n",
    "    # updated on Feb 21, 2021\n",
    "    my_dict[\"event_dict\"][event_id][\"tense\"] = tense_finder(my_dict['sentences'][sent_id][\"tense_list\"], event_dict[\"start_char\"] - my_dict['sentences'][sent_id][\"sent_start_char\"])\n",
    "    # updated on Oct 24, 2022, because of change of tense identification service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7a9d996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./MATRES_old/te3-platinum/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8f055b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [65,\n",
       "  1215,\n",
       "  566,\n",
       "  737,\n",
       "  112,\n",
       "  42421,\n",
       "  21294,\n",
       "  419,\n",
       "  363,\n",
       "  8814,\n",
       "  2065,\n",
       "  23547,\n",
       "  114,\n",
       "  66,\n",
       "  4481,\n",
       "  524,\n",
       "  2589,\n",
       "  1736,\n",
       "  21763,\n",
       "  16475,\n",
       "  17528,\n",
       "  1736,\n",
       "  11102,\n",
       "  2589,\n",
       "  566,\n",
       "  25504,\n",
       "  624,\n",
       "  968,\n",
       "  1762,\n",
       "  112,\n",
       "  391,\n",
       "  363,\n",
       "  1395,\n",
       "  569,\n",
       "  1404,\n",
       "  5400,\n",
       "  21763,\n",
       "  16475,\n",
       "  17528,\n",
       "  5400,\n",
       "  3189,\n",
       "  1404,\n",
       "  385,\n",
       "  488,\n",
       "  26501,\n",
       "  441,\n",
       "  420,\n",
       "  1912,\n",
       "  12533,\n",
       "  114,\n",
       "  66,\n",
       "  3963,\n",
       "  391,\n",
       "  858,\n",
       "  112,\n",
       "  440,\n",
       "  987,\n",
       "  1043,\n",
       "  114,\n",
       "  66],\n",
       " 'event_pos': [22, 40],\n",
       " 'event_pos_end': [23, 41],\n",
       " 'event_pair': [[1, 2]],\n",
       " 'labels': [0]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a61aa4",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69dfd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'transformers_model': 'google/bigbird-roberta-large',\n",
    "          'dataset': 'MATRES',   # 'HiEve', 'IC', 'MATRES' \n",
    "          'testdata': 'PRED', # MATRES / MATRES_nd / TDD / PRED / None; None means training mode\n",
    "          'block_size': 64,\n",
    "          'add_loss': 0, \n",
    "          'batch_size': 1,    # 6 works on 48G gpu\n",
    "          'epochs': 40,\n",
    "          'learning_rate': 5e-6,    # subject to change\n",
    "          'seed': 0,\n",
    "          'gpu_id': '11453',    # subject to change\n",
    "          'debug': 0,\n",
    "          'rst_file_name': '0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst',    # subject to change\n",
    "          'mask_in_input_ids': mask_in_input_ids,\n",
    "          'mask_in_input_mask': mask_in_input_mask,\n",
    "          'marker': 'abc', \n",
    "          'tense_acron': 0, # 1 (acronym of tense) or 0 (original tense)\n",
    "          't_marker': 1, # 2 (trigger enclosed by special tokens) or 1 (tense enclosed by **)\n",
    "          'td': 1, # 0 (no tense detection) or 1 (tense detection, add tense info)\n",
    "          'dpn': 0, # 1 if use DPN; else 0\n",
    "          'lambda_1': -10, # lower bound * 10\n",
    "          'lambda_2': 11, # upper bound * 10\n",
    "          'f1_metric': f1_metric, \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb84c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-large were not used when initializing BigBirdModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if acronym:\n",
    "    special_tokens_dict = {'additional_special_tokens': \n",
    "                           [' [futuperfsimp]',' [futucont]',' [futuperfcont]',' [futusimp]', ' [pastcont]', ' [pastperfcont]', ' [pastperfsimp]', ' [pastsimp]', ' [prescont]', ' [presperfcont]', ' [presperfsimp]', ' [pressimp]', ' [futuperfsimppass]',' [futucontpass]',' [futuperfcontpass]',' [futusimppass]', ' [pastcontpass]', ' [pastperfcontpass]', ' [pastperfsimppass]', ' [pastsimppass]', ' [prescontpass]', ' [presperfcontpass]', ' [presperfsimppass]', ' [pressimppass]', ' [none]'\n",
    "                           ]}\n",
    "    spec_toke_list = []\n",
    "    for t in special_tokens_dict['additional_special_tokens']:\n",
    "        spec_toke_list.append(\" [/\" + t[2:])\n",
    "    special_tokens_dict['additional_special_tokens'] += spec_toke_list\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model = AutoModel.from_pretrained(params['transformers_model'])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(params['transformers_model'])\n",
    "params['model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "977dc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(features_train, batch_size=params['batch_size'], shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "valid_dataloader = DataLoader(features_valid, batch_size=params['batch_size'], shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "test_dataloader = DataLoader(features_test, batch_size=params['batch_size'], shuffle=False, collate_fn=collate_fn, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b274d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 364635144\n"
     ]
    }
   ],
   "source": [
    "OnePassModel = transformers_mlp_cons(params)\n",
    "OnePassModel.to(cuda)\n",
    "OnePassModel.zero_grad()\n",
    "print(\"# of parameters:\", count_parameters(OnePassModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4189f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['cuda'] = cuda # not included in config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "271f3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_PATH = \"./\"\n",
    "best_1 = -0.4\n",
    "best_2 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2db857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['emb_size'] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd55b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_exp_test = exp(cuda, OnePassModel, params['epochs'], params['learning_rate'], \n",
    "                   train_dataloader, valid_dataloader, test_dataloader, \n",
    "                   params['dataset'], best_PATH, None, params['dpn'], model_name, None, [best_1, best_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1511bc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = rst_file_name.replace(\".rst\", \"\")\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a561937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 120 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 40 ========\n",
      "Training...\n",
      "  Batch    40  of  6,332.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  6,332.    Elapsed: 0:00:23.\n",
      "  Batch   120  of  6,332.    Elapsed: 0:00:35.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmem_exp_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/TORQUE/Faithful_TempRel/exp.py:112\u001b[0m, in \u001b[0;36mexp.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# weights update\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accum_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader)):\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Measure how long this epoch took.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.8/site-packages/torch/optim/adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.8/site-packages/torch/optim/_functional.py:87\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     86\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m---> 87\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mem_exp_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1c8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
