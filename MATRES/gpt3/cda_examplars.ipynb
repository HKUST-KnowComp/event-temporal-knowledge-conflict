{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "features_valid = json.load(open(\"../data/valid_subset_text.json\"))\n",
    "features_rest = json.load(open(\"../dataset_bias/valid_subset_rest_narrative.json\"))\n",
    "\n",
    "gen_examplars_res = np.load(\"../data/examplars_gda_rest.npy\", allow_pickle=True)\n",
    "\n",
    "def compare(d1, d2):\n",
    "    return all(d1[key] == d2[key] for key in d1)\n",
    "\n",
    "features_ori = [item for item in features_valid if all(not compare(d, item) for d in features_rest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_valid_tense_all = json.load(open(\"dataset_bias/valid_subset_text_tense_bias_vague.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a4e9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_valid_tense_all = json.load(open(\"../dataset_bias/valid_subset_text_tense_bias_vague.json\"))\n",
    "features_valid_erp = json.load(open(\"../dataset_bias/valid_subset_text_erp.json\"))\n",
    "features_valid_dep = json.load(open(\"../dataset_bias/valid_text_features_matres_dep_bias_subset.json\"))\n",
    "features_valid_narrative = json.load(open(\"../dataset_bias/valid_text_features_matres_narrative_bias_subset.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97558f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d132b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'event_pos', 'event_pos_end', 'event_pair', 'labels'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_valid_narrative[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a686148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-large\")\n",
    "features_valid_narrative_text = [{'text':tokenizer.decode(item['input_ids']), \n",
    "                                  'e1':tokenizer.decode(item['input_ids'][item['event_pos'][0]]),\n",
    "                                  'e2':tokenizer.decode(item['input_ids'][item['event_pos'][1]]), \n",
    "                                  'labels':item['labels']} for item in features_valid_narrative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f66a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset_bias/valid_subset_text_narrative.json\", \"w\") as writer:\n",
    "    json.dump(features_valid_narrative_text, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c2b2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "erp_examplars = np.load(\"../dataset_bias/gpt3_data/valid_gpt2xl_hard_gpt3_examplars.npy\", allow_pickle=True)\n",
    "tense_examplars = np.load(\"../dataset_bias/gpt3_data/tense_gpt3_examplars_all.npy\", allow_pickle=True)\n",
    "narrative_examplars = np.load(\"../data/examplars_gda_narrative_bias.npy\", allow_pickle=True)\n",
    "dep_examplars = np.load(\"../data/examplars_gda_dep_bias.npy\", allow_pickle=True)\n",
    "rest_examplars = np.load(\"../data/examplars_gda_rest.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c28b38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "477"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(narrative_examplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f124b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3171"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_valid_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14515b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examplars = [[\"\"] for i in range(len(features_valid))]\n",
    "rest_2_all_idx = []\n",
    "\n",
    "for i in range(len(features_rest)):\n",
    "    rest_2_all_idx.append([])\n",
    "    for j in range(len(features_valid)):\n",
    "        if compare(features_valid[j], features_rest[i]):\n",
    "            rest_2_all_idx[-1].append(j)\n",
    "for i in range(len(features_rest)):\n",
    "    for idx in rest_2_all_idx[i]:\n",
    "        all_examplars[idx] = rest_examplars[i]\n",
    "        \n",
    "erp_2_all_idx = []\n",
    "for i in range(len(features_valid_erp)):\n",
    "    erp_2_all_idx.append([])\n",
    "    for j in range(len(features_valid)):\n",
    "        if compare(features_valid[j], features_valid_erp[i]):\n",
    "            erp_2_all_idx[-1].append(j)\n",
    "for i in range(len(features_valid_erp)):\n",
    "    for idx in erp_2_all_idx[i]:\n",
    "        all_examplars[idx] = erp_examplars[i]\n",
    "        \n",
    "tense_2_all_idx = []\n",
    "for i in range(len(features_valid_tense_all)):\n",
    "    tense_2_all_idx.append([])\n",
    "    for j in range(len(features_valid)):\n",
    "        if compare(features_valid[j], features_valid_tense_all[i]):\n",
    "            tense_2_all_idx[-1].append(j)\n",
    "for i in range(len(features_valid_erp)):\n",
    "    for idx in tense_2_all_idx[i]:\n",
    "        all_examplars[idx] = tense_examplars[i]\n",
    "\n",
    "narrative_2_all_idx = []\n",
    "for i in range(len(features_valid_narrative)):\n",
    "    narrative_2_all_idx.append([])\n",
    "    for j in range(len(features_valid)):\n",
    "        if compare(features_valid[j], features_valid_narrative[i]):\n",
    "            narrative_2_all_idx[-1].append(j)\n",
    "for i in range(len(features_valid_narrative)):\n",
    "    for idx in narrative_2_all_idx[i]:\n",
    "        all_examplars[idx] = narrative_examplars[i]\n",
    "        \n",
    "dep_2_all_idx = []\n",
    "for i in range(len(features_valid_dep)):\n",
    "    dep_2_all_idx.append([])\n",
    "    for j in range(len(features_valid)):\n",
    "        if compare(features_valid[j], features_valid_dep[i]):\n",
    "            dep_2_all_idx[-1].append(j)\n",
    "for i in range(len(features_valid_dep)):\n",
    "    for idx in dep_2_all_idx[i]:\n",
    "        all_examplars[idx] = dep_examplars[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da6bd4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examplars[0][0].find('\\n\\nQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "658b7e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'waiting\" and \"said\"? \\nChoice A: waiting happens before said. \\nChoice B: waiting happens after said. \\nChoice C: waiting happens during said. \\nChoice D: unknown. \\nAnswer only with A, B, C, or D. \\n\\nA: Choice B'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examplars[0][1][all_examplars[0][1].find('\\n\\nQ'):][len('\\n\\nQ: What\\'s the temporal relation between the event \"'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2de22632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_examplar(text):\n",
    "    prefix_context = 'Given the context:\\n'\n",
    "    suffix_context = '\\n\\nQ:'\n",
    "    context = text[len(prefix_context):text.find(suffix_context)]\n",
    "\n",
    "    rest = text[text.find(suffix_context):][len('\\n\\nQ: What\\'s the temporal relation between the event \"'):]\n",
    "\n",
    "    e1 = rest.split('\"')[0]\n",
    "    e2 = rest.split('\"')[2]\n",
    "    labels = rest[-1]\n",
    "\n",
    "    assert labels in ['A', 'B', 'C', 'D']\n",
    "    \n",
    "    return {'context':context, 'e1':e1, 'e2':e2, 'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d85e6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examplars_raw= []\n",
    "for items in all_examplars:\n",
    "    tmp_examplars = []\n",
    "    for item in items:\n",
    "        tmp_examplars.append(parse_examplar(item))\n",
    "    all_examplars_raw.append(tmp_examplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6467f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc24c8f",
   "metadata": {},
   "source": [
    "# generate examplars for the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c20e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e82384",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2letter = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\"}\n",
    "\n",
    "saved_gpt3_examplars_0 = []\n",
    "\n",
    "for i, item in tqdm(enumerate(features_ori), total=len(features_ori)):\n",
    "\n",
    "    context = item['text'].replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
    "    e1 = item['e1']\n",
    "    e2 = item['e2']\n",
    "    \n",
    "    counter_factual_prompt = [f\"Generate a paragraph where event {e1} happens before {e2}:\", \n",
    "                              f\"Generate a paragraph where event {e1} happens after {e2}:\", \n",
    "                              f\"Generate a paragraph where event {e1} happens in the same time as {e2}:\", \n",
    "                              f\"Generate a paragraph where the temporal relation of {e1} and {e2} cannot be determined based on the context:\", \n",
    "                             ]\n",
    "    gpt3_context = []\n",
    "    examplars = []\n",
    "    for l, prompt in zip([0, 1, 2, 3], counter_factual_prompt):\n",
    "        while True:\n",
    "            try:\n",
    "                gpt3_context.append(\n",
    "                    openai.Completion.create(\n",
    "                                model=\"text-davinci-003\",\n",
    "                                prompt=prompt,\n",
    "                                max_tokens=40,\n",
    "                                temperature=0\n",
    "                    )[\"choices\"][0][\"text\"].strip()\n",
    "                )\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(20)\n",
    "        time.sleep(2)\n",
    "        examplars.append(\"Given the context:\\n\" + gpt3_context[-1] + f\"\\n\\nQ: What's the temporal relation between the event \\\"{e1}\\\" and \\\"{e2}\\\"? \\nChoice A: {e1} happens before {e2}. \\nChoice B: {e1} happens after {e2}. \\nChoice C: {e1} happens during {e2}. \\nChoice D: unknown. \\nAnswer only with A, B, C, or D. \\n\\nA: Choice {label2letter[l]}\")\n",
    "    \n",
    "    saved_gpt3_examplars_0.append(examplars)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e8d0e",
   "metadata": {},
   "source": [
    "## use the examplars to do CDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7d53da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "zs_preds = json.load(open(\"./results/template_2_zeroshot_pred.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3da7ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_2_num = {'A':0,'B':1,'C':2,'D':3}\n",
    "convert_dict_rev = {0:'BEFORE', 1:'AFTER', 2:'EQUAL', 3:'VAGUE'}\n",
    "def construct_icl_examplar(item):\n",
    "    context = item['context']\n",
    "    e1 = item['e1']\n",
    "    e2 = item['e2']\n",
    "    label = choice_2_num[item['labels']]\n",
    "    prompt = f\"Determine the temporal order from \\\"{e1}\\\" to \\\"{e2}\\\" in the following sentence: \\\"{context}\\\". Only answer one word from AFTER, BEFORE, EQUAL, VAGUE. Answer: {convert_dict_rev[label]}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fb22babf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'The crowd was eagerly waiting for the event to begin. Everyone was filled with anticipation and excitement as they waited for the curtains to open. The atmosphere was electric as people chatted and laughed,',\n",
       " 'e1': 'waiting',\n",
       " 'e2': 'said',\n",
       " 'labels': 'A'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examplars_raw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9fd4e076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Determine the temporal order from \"waiting\" to \"said\" in the following sentence: \"The crowd was eagerly waiting for the event to begin. Everyone was filled with anticipation and excitement as they waited for the curtains to open. The atmosphere was electric as people chatted and laughed,\". Only answer one word from AFTER, BEFORE, EQUAL, VAGUE. Answer: BEFORE'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_icl_examplar(all_examplars_raw[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48872bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "38bf9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [53:32<00:00,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "all_gens_cda = []\n",
    "for i, item in tqdm(enumerate(features_valid), total=len(features_valid)):\n",
    "    context = item['text'].replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
    "    e1 = item['e1']\n",
    "    e2 = item['e2']\n",
    "    \n",
    "\n",
    "    examplars = [construct_icl_examplar(all_examplars_raw[i][_]) for _ in range(4) if _ != zs_preds[i]]\n",
    "    examplar = \"\\n\\n\".join(examplars)\n",
    "    \n",
    "    prompt = f\"Determine the temporal order from \\\"{e1}\\\" to \\\"{e2}\\\" in the following sentence: \\\"{context}\\\". Only answer one word from AFTER, BEFORE, EQUAL, VAGUE. \"\n",
    "#     print(prompt)\n",
    "    while True:\n",
    "        try:\n",
    "            all_gens_cda.append(openai.Completion.create(\n",
    "                        model=\"text-davinci-003\",\n",
    "                        prompt=examplar + \"\\n\\n\" + prompt,\n",
    "                        max_tokens=20,\n",
    "                        temperature=0\n",
    "            ))\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "    time.sleep(2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3622643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e47f3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {'BEFORE':0, 'AFTER':1, 'EQUAL':2, 'VAGUE':3, 'EQU':2}\n",
    "def parse_result(ans):\n",
    "    return convert_dict.get(ans[len(\"Answer: \"):].upper(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "82bfb106",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cda = [parse_result(gen[\"choices\"][0][\"text\"].strip()) for gen in all_gens_cda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4fac413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Answer: BEFORE': 883,\n",
       "         'Answer: AFTER': 93,\n",
       "         'Answer: V': 21,\n",
       "         'Answer: EQU': 3})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([gen[\"choices\"][0][\"text\"].strip() for gen in all_gens_cda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a199b959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 883, 1: 93, 3: 21, 2: 3})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(results_cda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb55fe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19850458861631082 0.472\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = [features_valid[i]['labels'][0] for i in range(len(features_valid))]\n",
    "results_cda\n",
    "print(f1_score(labels, results_cda, average='macro'), f1_score(labels, results_cda, average=\"micro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/template_1_threeshot_subsets_pred_2.json\", \"w\") as writer:\n",
    "#     json.dump(threeshot_preds_2, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a866671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf2e7bd",
   "metadata": {},
   "source": [
    "# newly generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb1fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1368441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examplars_conflict = np.load(\"../data/examplars_gda_conflict.npy\", allow_pickle=True)\n",
    "all_examplars_rest = np.load(\"../data/examplars_gda_rest.npy\", allow_pickle=True)\n",
    "\n",
    "all_examplars_new = [[] for i in range(len(features_valid))]\n",
    "\n",
    "rest_2_all_idx = []\n",
    "\n",
    "for i in range(len(features_rest)):\n",
    "    rest_2_all_idx.append([])\n",
    "    for j in range(len(features_valid)):\n",
    "        if compare(features_valid[j], features_rest[i]):\n",
    "            rest_2_all_idx[-1].append(j)\n",
    "for i in range(len(features_rest)):\n",
    "    for idx in rest_2_all_idx[i]:\n",
    "        all_examplars_new[idx] = all_examplars_rest[i]\n",
    "        \n",
    "conflict_2_all_idx = []\n",
    "\n",
    "for i in range(len(features_ori)):\n",
    "    conflict_2_all_idx.append([])\n",
    "    for j in range(len(features_valid)):\n",
    "        if compare(features_valid[j], features_ori[i]):\n",
    "            conflict_2_all_idx[-1].append(j)\n",
    "for i in range(len(features_ori)):\n",
    "    for idx in conflict_2_all_idx[i]:\n",
    "        all_examplars_new[idx] = all_examplars_conflict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dccd1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examplars_raw_new= []\n",
    "for items in all_examplars_new:\n",
    "    tmp_examplars = []\n",
    "    for item in items:\n",
    "        tmp_examplars.append(parse_examplar(item))\n",
    "    all_examplars_raw_new.append(tmp_examplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "909c5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examplars_raw_new == all_examplars_raw\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for item0, item1 in zip(all_examplars_raw_new, all_examplars_raw):\n",
    "    for option0, option1 in zip(item0, item1):\n",
    "        if not all(option0[key] == option1[key] for key in option0):\n",
    "            # print(option0, option1)\n",
    "            cnt += 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8fc1c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "option0=all_examplars_raw_new[0][0] \n",
    "option1 = all_examplars_raw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aef99152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(option0[key] == option1[key] for key in option0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b970f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e97a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "298bb77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [47:03<00:00,  2.82s/it]\n"
     ]
    }
   ],
   "source": [
    "all_gens_cda_1 = []\n",
    "for i, item in tqdm(enumerate(features_valid), total=len(features_valid)):\n",
    "    context = item['text'].replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
    "    e1 = item['e1']\n",
    "    e2 = item['e2']\n",
    "    \n",
    "\n",
    "    examplars = [construct_icl_examplar(all_examplars_raw_new[i][_]) for _ in range(4) if _ != zs_preds[i]]\n",
    "    examplar = \"\\n\\n\".join(examplars)\n",
    "    \n",
    "    prompt = f\"Determine the temporal order from \\\"{e1}\\\" to \\\"{e2}\\\" in the following sentence: \\\"{context}\\\". Only answer one word from AFTER, BEFORE, EQUAL, VAGUE. \"\n",
    "#     print(prompt)\n",
    "    while True:\n",
    "        try:\n",
    "            all_gens_cda_1.append(openai.Completion.create(\n",
    "                        model=\"text-davinci-003\",\n",
    "                        prompt=examplar + \"\\n\\n\" + prompt,\n",
    "                        max_tokens=20,\n",
    "                        temperature=0\n",
    "            ))\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "    time.sleep(2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5c300eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cda_1 = [parse_result(gen[\"choices\"][0][\"text\"].strip()) for gen in all_gens_cda_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7cd3ac54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 910, 1: 73, 3: 14, 2: 3})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(results_cda_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0f15a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19786903857278318 0.485\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels, results_cda_1, average='macro'), f1_score(labels, results_cda_1, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b449e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7774c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5082b2c1",
   "metadata": {},
   "source": [
    "# template 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ab53a500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Given the context:\\nThe crowd was eagerly waiting for the event to begin. Everyone was filled with anticipation and excitement as they waited for the curtains to open. The atmosphere was electric as people chatted and laughed,\\n\\nQ: What\\'s the temporal relation between the event \"waiting\" and \"said\"? \\nChoice A: waiting happens before said. \\nChoice B: waiting happens after said. \\nChoice C: waiting happens during said. \\nChoice D: unknown. \\nAnswer only with A, B, C, or D. \\n\\nA: Choice A',\n",
       "       'Given the context:\\nThe event was finally here. Everyone had been waiting for weeks, and the anticipation was palpable. As the clock ticked closer to the start time, the crowd grew more and more excited.\\n\\nQ: What\\'s the temporal relation between the event \"waiting\" and \"said\"? \\nChoice A: waiting happens before said. \\nChoice B: waiting happens after said. \\nChoice C: waiting happens during said. \\nChoice D: unknown. \\nAnswer only with A, B, C, or D. \\n\\nA: Choice B',\n",
       "       'Given the context:\\nThe event was in full swing, with people waiting in anticipation for the big announcement. Music was playing in the background, and the atmosphere was electric. Everyone was chatting and laughing, and the\\n\\nQ: What\\'s the temporal relation between the event \"waiting\" and \"said\"? \\nChoice A: waiting happens before said. \\nChoice B: waiting happens after said. \\nChoice C: waiting happens during said. \\nChoice D: unknown. \\nAnswer only with A, B, C, or D. \\n\\nA: Choice C',\n",
       "       'Given the context:\\n\"I said I would wait, but I\\'m not sure how long it will take. I\\'m sure it won\\'t be too long, but I\\'m not sure when I\\'ll hear back\\n\\nQ: What\\'s the temporal relation between the event \"waiting\" and \"said\"? \\nChoice A: waiting happens before said. \\nChoice B: waiting happens after said. \\nChoice C: waiting happens during said. \\nChoice D: unknown. \\nAnswer only with A, B, C, or D. \\n\\nA: Choice D'],\n",
       "      dtype='<U552')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examplars_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b4776077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [45:46<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "all_gens_cda_template_1 = []\n",
    "for i, item in tqdm(enumerate(features_valid), total=len(features_valid)):\n",
    "    context = item['text'].replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
    "    e1 = item['e1']\n",
    "    e2 = item['e2']\n",
    "    \n",
    "\n",
    "    examplars = [all_examplars_new[i][_] for _ in range(4) if _ != zs_preds[i]]\n",
    "    examplar = \"\\n\\n\".join(examplars)\n",
    "    \n",
    "    prompt = \"Given the context:\\n\" + context + f\"\\n\\nQ: What's the temporal relation between the event \\\"{e1}\\\" and \\\"{e2}\\\"? \\nChoice A: {e1} happens before {e2}. \\nChoice B: {e1} happens after {e2}. \\nChoice C: {e1} happens during {e2}. \\nChoice D: unknown. \\nAnswer only with A, B, C, or D. \\n\\nA: Choice \"\n",
    "#     print(prompt)\n",
    "    while True:\n",
    "        try:\n",
    "            all_gens_cda_template_1.append(openai.Completion.create(\n",
    "                        model=\"text-davinci-003\",\n",
    "                        prompt=examplar + \"\\n\\n\" + prompt,\n",
    "                        max_tokens=20,\n",
    "                        temperature=0\n",
    "            ))\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0b94c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3061144839549003 0.495\n"
     ]
    }
   ],
   "source": [
    "convert_dict = {'A':0, 'B':1, 'C':2, 'D':3}\n",
    "convert_dict_rev = {0:'A', 1:'B', 2:'C', 3:'D'}\n",
    "def parse_result(ans):\n",
    "    return convert_dict.get(ans.upper(), 3)\n",
    "\n",
    "\n",
    "template_1_preds_1 =  [parse_result(item['choices'][0]['text'].strip()) for item in all_gens_cda_template_1]\n",
    "\n",
    "print(f1_score(labels, template_1_preds_1, average='macro'), \n",
    "      f1_score(labels, template_1_preds_1, average=\"micro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
