{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37fd537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 03/09/2023 17:37:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",,
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime \n",
    "import random\n",
    "from matres_reader_with_tense import *\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import DataLoader\n",
    "from util import *\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from model import transformers_mlp_cons\n",
    "from exp import *\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from synonyms import *\n",
    "import pickle\n",
    "from timeline_construct import *\n",
    "from ts import func, ModelWithTemperature\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# datetime object containing current date and time\n",
    "now = datetime.datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "\n",
    "#label_dict={\"SuperSub\": 0, \"SubSuper\": 1, \"Coref\": 2, \"NoRel\": 3}\n",
    "num_dict = {0: \"before\", 1: \"after\", 2: \"equal\", 3: \"vague\"}\n",
    "#def label_to_num(label):\n",
    "#    return label_dict[label]\n",
    "def num_to_label(num):\n",
    "    return num_dict[num]\n",
    "\n",
    "mask_in_input_ids = 0 # note that [MASK] is actually learned through pre-training\n",
    "mask_in_input_mask = 0 # when input is masked through attention, it would be replaced with [PAD]\n",
    "acronym = 0 # using acronym for tense (e.g., pastsimp): 1; else (e.g., past simple): 0\n",
    "t_marker = 1\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "def docTransformerTokenIDs(sentences):\n",
    "    if len(sentences) < 1:\n",
    "        return None\n",
    "    elif len(sentences) == 1:\n",
    "        return sentences[0]['_subword_to_ID']\n",
    "    else:\n",
    "        TokenIDs = sentences[0]['_subword_to_ID']\n",
    "        for i in range(1, len(sentences)):\n",
    "            TokenIDs += sentences[i]['_subword_to_ID'][1:]\n",
    "        return TokenIDs\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    max_len = max([len(f['input_ids']) for f in batch])\n",
    "    input_ids = [f['input_ids'] + [0] * (max_len - len(f['input_ids'])) for f in batch]\n",
    "    if mask_in_input_ids:\n",
    "        input_ids_new = []\n",
    "        for f_id, f in enumerate(input_ids):\n",
    "            for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "                end = batch[f_id]['event_pos_end'][event_id]\n",
    "                for token_id in range(start, end): # needs verification\n",
    "                    f[token_id] = 67\n",
    "            input_ids_new.append(f)\n",
    "        input_ids = input_ids_new\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    input_mask = [[1.0] * len(f['input_ids']) + [0.0] * (max_len - len(f['input_ids'])) for f in batch]\n",
    "    if mask_in_input_mask:\n",
    "        input_mask_new = []\n",
    "        for f_id, f in enumerate(input_mask):\n",
    "            for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "                end = batch[f_id]['event_pos_end'][event_id]\n",
    "                for token_id in range(start, end): # needs verification\n",
    "                    f[token_id] = 0.0\n",
    "            input_mask_new.append(f)\n",
    "        input_mask = input_mask_new\n",
    "    # Updated on May 17, 2022    \n",
    "    input_mask_eo = [[0.0] * max_len for f in batch]\n",
    "    for f_id, f in enumerate(input_mask_eo):\n",
    "        for event_id, start in enumerate(batch[f_id]['event_pos']):\n",
    "            end = batch[f_id]['event_pos_end'][event_id]\n",
    "            for token_id in range(start, end): # needs verification\n",
    "                f[token_id] = 1.0\n",
    "    # Updated on Jun 14, 2022\n",
    "    input_mask_xbar = [[0.0] * max_len for f in batch]\n",
    "    input_mask_xbar = torch.tensor(input_mask_xbar, dtype=torch.float)\n",
    "    input_mask_eo = torch.tensor(input_mask_eo, dtype=torch.float)\n",
    "    input_mask = torch.tensor(input_mask, dtype=torch.float)\n",
    "    event_pos = [f['event_pos'] for f in batch]\n",
    "    event_pos_end = [f['event_pos_end'] for f in batch]\n",
    "    event_pair = [f['event_pair'] for f in batch]\n",
    "    labels = [f['labels'] for f in batch]\n",
    "    output = (input_ids, input_mask, event_pos, event_pos_end, event_pair, labels, input_mask_eo, input_mask_xbar)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45865614",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = 'micro'\n",
    "params = {'transformers_model': 'google/bigbird-roberta-large',\n",
    "          'dataset': 'MATRES',   # 'HiEve', 'IC', 'MATRES' \n",
    "          'testdata': 'None', # MATRES / MATRES_nd / TDD / PRED / None; None means training mode\n",
    "          'block_size': 64,\n",
    "          'add_loss': 0, \n",
    "          'batch_size': 1,    # 6 works on 48G gpu\n",
    "          'epochs': 40,\n",
    "          'learning_rate': 5e-6,    # subject to change\n",
    "          'seed': 0,\n",
    "          'gpu_id': '11453',    # subject to change\n",
    "          'debug': 0,\n",
    "          'rst_file_name': '0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst',    # subject to change\n",
    "          'mask_in_input_ids': mask_in_input_ids,\n",
    "          'mask_in_input_mask': mask_in_input_mask,\n",
    "          'marker': 'abc', \n",
    "          'tense_acron': 0, # 1 (acronym of tense) or 0 (original tense)\n",
    "          't_marker': 1, # 2 (trigger enclosed by special tokens) or 1 (tense enclosed by **)\n",
    "          'td': 1, # 0 (no tense detection) or 1 (tense detection, add tense info)\n",
    "          'dpn': 1, # 1 if use DPN; else 0\n",
    "          'lambda_1': -10, # lower bound * 10\n",
    "          'lambda_2': 11, # upper bound * 10\n",
    "          'f1_metric': f1_metric, \n",
    "         }\n",
    "# $acr $tmarker $td $dpn $mask $lambda_1 $lambda_2\n",
    "\n",
    "# FOR 48GBgpu\n",
    "if params['testdata'] in ['MATRES', 'MATRES_nd']:\n",
    "    #params['batch_size'] = 400\n",
    "    params['batch_size'] = 1\n",
    "if params['testdata'] in ['TDD']:\n",
    "    params['batch_size'] = 100\n",
    "    \n",
    "if params['testdata'] == 'MATRES_nd':\n",
    "    params['nd'] = True\n",
    "else:\n",
    "    params['nd'] = False\n",
    "    \n",
    "###########\n",
    "# NO MASK #\n",
    "###########\n",
    "\n",
    "if params['rst_file_name'] == '0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst':\n",
    "    slurm_id = '11453'\n",
    "#params['rst_file_name'] = '0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst' \n",
    "#slurm_id = '11453'\n",
    "# python main_pair.py 0615_11453.rst 5e-6 400 11453 0 MATRES abc 0 1 1 1 0 -10 11\n",
    "# python main_pair.py 0511pm-lr5e-6-b20-gpu9942-loss0-dataMATRES-accum1-marker@**@-pair1-acr0-tmarker1-td1-dpn1-mask0.rst 5e-6 400 11453 0 MATRES abc 0 1 1 1 0 -10 11\n",
    "\n",
    "if params['transformers_model'][-5:] == \"large\":\n",
    "    params['emb_size'] = 1024\n",
    "elif params['transformers_model'][-4:] == \"base\":\n",
    "    params['emb_size'] = 768\n",
    "else:\n",
    "    print(\"emb_size is neither 1024 nor 768? ...\")\n",
    "    \n",
    "set_seed(params['seed'])\n",
    "rst_file_name = params['rst_file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ea108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_PATH = sys.argv[1] + '/' + '0511.pt' \n",
    "model_name = rst_file_name.replace(\".rst\", \"\")\n",
    "with open(\"config/\" + rst_file_name.replace(\"rst\", \"json\"), 'w') as config_file:\n",
    "    json.dump(params, config_file)\n",
    "    \n",
    "if int(params['gpu_id']) < 10:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = params['gpu_id']\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "cuda = torch.device('cuda')\n",
    "params['cuda'] = cuda # not included in config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2e17ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MATRES dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-large were not used when initializing BigBirdModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing \" + params['dataset'] + \" dataset...\")\n",
    "t0 = time.time()\n",
    "if params['dataset'] == \"IC\":\n",
    "    dir_name = \"./IC/IC_Processed/\"\n",
    "    #max_sent_len = 193\n",
    "elif params['dataset'] == \"HiEve\":\n",
    "    dir_name = \"./hievents_v2/processed/\"\n",
    "    #max_sent_len = 155\n",
    "elif params['dataset'] == \"MATRES\":\n",
    "    dir_name = \"\"\n",
    "else:\n",
    "    print(\"Not supporting this dataset yet!\")\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(params['transformers_model'])   \n",
    "if acronym:\n",
    "    special_tokens_dict = {'additional_special_tokens': \n",
    "                           [' [futuperfsimp]',' [futucont]',' [futuperfcont]',' [futusimp]', ' [pastcont]', ' [pastperfcont]', ' [pastperfsimp]', ' [pastsimp]', ' [prescont]', ' [presperfcont]', ' [presperfsimp]', ' [pressimp]', ' [futuperfsimppass]',' [futucontpass]',' [futuperfcontpass]',' [futusimppass]', ' [pastcontpass]', ' [pastperfcontpass]', ' [pastperfsimppass]', ' [pastsimppass]', ' [prescontpass]', ' [presperfcontpass]', ' [presperfsimppass]', ' [pressimppass]', ' [none]'\n",
    "                           ]}\n",
    "    spec_toke_list = []\n",
    "    for t in special_tokens_dict['additional_special_tokens']:\n",
    "        spec_toke_list.append(\" [/\" + t[2:])\n",
    "    special_tokens_dict['additional_special_tokens'] += spec_toke_list\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model = AutoModel.from_pretrained(params['transformers_model'])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(params['transformers_model'])\n",
    "params['model'] = model\n",
    "debug = params['debug']\n",
    "if debug:\n",
    "    params['epochs'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa3b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tense_info(x_sent, tense, start, mention, special_1, special_2):\n",
    "    # x:\n",
    "    # special_1: 2589\n",
    "    # special_2: 1736\n",
    "    \n",
    "    # y:\n",
    "    # special_1: 1404\n",
    "    # special_2: 5400\n",
    "    orig_len = len(x_sent)        \n",
    "    if tense:\n",
    "        if acronym:\n",
    "            tense_marker = tokenizer.encode(\" \" + tense[acronym])[1:-1]\n",
    "        else:\n",
    "            tense_marker = tokenizer.encode(tense[acronym])[1:-1]\n",
    "    else:\n",
    "        if acronym:\n",
    "            tense_marker = tokenizer.encode(\" [none]\")[1:-1]\n",
    "        else:\n",
    "            tense_marker = tokenizer.encode(\"None\")[1:-1]\n",
    "    subword_len = len(tokenizer.encode(mention)) - 2\n",
    "    if t_marker == 2:\n",
    "        # trigger enclosed by special tense tokens\n",
    "        assert acronym == 1\n",
    "        x_sent = x_sent[0:start] + tense_marker + x_sent[start:start+subword_len] + tokenizer.encode(\" [/\" + tokenizer.decode(tense_marker)[2:])[1:-1] + x_sent[start+subword_len:]\n",
    "        new_start = start + len(tense_marker)\n",
    "    elif t_marker == 1:\n",
    "        # tense enclosed by * *\n",
    "        x_sent = x_sent[0:start] + [special_1, special_2] + tense_marker + [special_2] + x_sent[start:start+subword_len] + [special_1] + x_sent[start+subword_len:]\n",
    "        new_start = start + len([special_1, special_2] + tense_marker + [special_2])\n",
    "    new_end = new_start + subword_len\n",
    "    offset = len(x_sent) - orig_len\n",
    "    return x_sent, offset, new_start, new_end\n",
    "\n",
    "def reverse_num(event_position):\n",
    "    return [event_position[1], event_position[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1317f038",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eiid_pair_to_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m context_len \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m timeline_input \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[43meiid_pair_to_label\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m     18\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m fname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m onlyfiles_TB:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eiid_pair_to_label' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_id = -1\n",
    "features_train = []\n",
    "features_valid = []\n",
    "features_test = []\n",
    "t0 = time.time()\n",
    "relation_stats = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "t_marker = params['t_marker']\n",
    "# 2: will [futusimp] begin [/futusimp]\n",
    "# 1: will @ * Future Simple * begin @ \n",
    "\n",
    "max_len = 0\n",
    "sent_num = 0\n",
    "pair_num = 0\n",
    "test_labels = []\n",
    "context_len = {}\n",
    "timeline_input = []\n",
    "for fname in tqdm.tqdm(eiid_pair_to_label.keys()):\n",
    "    file_name = fname + \".tml\"\n",
    "    if file_name in onlyfiles_TB:\n",
    "        dir_name = mypath_TB\n",
    "    elif file_name in onlyfiles_AQ:\n",
    "        dir_name = mypath_AQ\n",
    "    elif file_name in onlyfiles_PL:\n",
    "        dir_name = mypath_PL\n",
    "    else:\n",
    "        continue\n",
    "    my_dict = tml_reader(dir_name, file_name, tokenizer) \n",
    "    \n",
    "    for (eiid1, eiid2) in eiid_pair_to_label[fname].keys():\n",
    "        pair_num += 1\n",
    "        event_pos = []\n",
    "        event_pos_end = []\n",
    "        relations = []\n",
    "        TokenIDs = [65]\n",
    "        x = my_dict[\"eiid_dict\"][eiid1][\"eID\"] # eID\n",
    "        y = my_dict[\"eiid_dict\"][eiid2][\"eID\"]\n",
    "        x_sent_id = my_dict[\"event_dict\"][x][\"sent_id\"]\n",
    "        y_sent_id = my_dict[\"event_dict\"][y][\"sent_id\"]\n",
    "        reverse = False\n",
    "        if x_sent_id > y_sent_id:\n",
    "            reverse = True\n",
    "            x = my_dict[\"eiid_dict\"][eiid2][\"eID\"]\n",
    "            y = my_dict[\"eiid_dict\"][eiid1][\"eID\"]\n",
    "            x_sent_id = my_dict[\"event_dict\"][x][\"sent_id\"]\n",
    "            y_sent_id = my_dict[\"event_dict\"][y][\"sent_id\"]\n",
    "        elif x_sent_id == y_sent_id:\n",
    "            x_position = my_dict[\"event_dict\"][x][\"_subword_id\"]\n",
    "            y_position = my_dict[\"event_dict\"][y][\"_subword_id\"]\n",
    "            if x_position > y_position:\n",
    "                reverse = True\n",
    "                x = my_dict[\"eiid_dict\"][eiid2][\"eID\"]\n",
    "                y = my_dict[\"eiid_dict\"][eiid1][\"eID\"]\n",
    "        x_sent = my_dict[\"sentences\"][x_sent_id][\"_subword_to_ID\"]\n",
    "        y_sent = my_dict[\"sentences\"][y_sent_id][\"_subword_to_ID\"]\n",
    "        # This guarantees that trigger x is always before trigger y in narrative order\n",
    "\n",
    "        context_start_sent_id = max(x_sent_id-1, 0)\n",
    "        context_end_sent_id = min(y_sent_id+2, len(my_dict[\"sentences\"]))\n",
    "        c_len = context_end_sent_id - context_start_sent_id\n",
    "        if c_len in context_len.keys():\n",
    "            context_len[c_len] += 1\n",
    "        else:\n",
    "            context_len[c_len] = 1\n",
    "        sent_num += c_len\n",
    "        \n",
    "        if params['td'] == 1:\n",
    "            x_sent, offset_x, new_start_x, new_end_x = add_tense_info(x_sent, my_dict[\"event_dict\"][x]['tense'], my_dict['event_dict'][x]['_subword_id'], my_dict[\"event_dict\"][x]['mention'], 2589, 1736)\n",
    "        else:\n",
    "            x_sent, offset_x, new_start_x, new_end_x = x_sent, 0, my_dict['event_dict'][x]['_subword_id'], my_dict['event_dict'][x]['_subword_id'] + len(tokenizer.encode(my_dict[\"event_dict\"][x]['mention'])) - 2\n",
    "            \n",
    "        if x_sent_id != y_sent_id:\n",
    "            if params['td'] == 1:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = add_tense_info(y_sent, my_dict[\"event_dict\"][y]['tense'], my_dict['event_dict'][y]['_subword_id'], my_dict[\"event_dict\"][y]['mention'], 1404, 5400)\n",
    "            else:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = y_sent, 0, my_dict['event_dict'][y]['_subword_id'], my_dict['event_dict'][y]['_subword_id'] + len(tokenizer.encode(my_dict[\"event_dict\"][y]['mention'])) - 2\n",
    "            for sid in range(context_start_sent_id, context_end_sent_id):\n",
    "                if sid == x_sent_id:\n",
    "                    event_pos.append(new_start_x + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_x + len(TokenIDs) - 1)\n",
    "                    TokenIDs += x_sent[1:]\n",
    "                elif sid == y_sent_id:\n",
    "                    event_pos.append(new_start_y + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_y + len(TokenIDs) - 1)\n",
    "                    TokenIDs += y_sent[1:]\n",
    "                else:\n",
    "                    TokenIDs += my_dict[\"sentences\"][sid][\"_subword_to_ID\"][1:]\n",
    "        else:\n",
    "            if params['td'] == 1:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = add_tense_info(x_sent, my_dict[\"event_dict\"][y]['tense'], my_dict['event_dict'][y]['_subword_id'] + offset_x, my_dict[\"event_dict\"][y]['mention'], 1404, 5400)\n",
    "            else:\n",
    "                y_sent, offset_y, new_start_y, new_end_y = y_sent, 0, my_dict['event_dict'][y]['_subword_id'], my_dict['event_dict'][y]['_subword_id'] + len(tokenizer.encode(my_dict[\"event_dict\"][y]['mention'])) - 2\n",
    "            for sid in range(context_start_sent_id, context_end_sent_id):\n",
    "                if sid == y_sent_id:\n",
    "                    event_pos.append(new_start_x + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_x + len(TokenIDs) - 1)\n",
    "                    event_pos.append(new_start_y + len(TokenIDs) - 1)\n",
    "                    event_pos_end.append(new_end_y + len(TokenIDs) - 1)\n",
    "                    TokenIDs += y_sent[1:]\n",
    "                else:\n",
    "                    TokenIDs += my_dict[\"sentences\"][sid][\"_subword_to_ID\"][1:]\n",
    "                    \n",
    "        if reverse:\n",
    "            event_pos = reverse_num(event_pos)\n",
    "            event_pos_end = reverse_num(event_pos_end)\n",
    "            \n",
    "        xy = eiid_pair_to_label[fname][(eiid1, eiid2)]\n",
    "        \n",
    "        relations.append(xy)\n",
    "        relation_stats[xy] += 1\n",
    "        if len(TokenIDs) > max_len:\n",
    "            max_len = len(TokenIDs)\n",
    "        \n",
    "        if debug or pair_num < 5:\n",
    "            print(\"first event of the pair:\", tokenizer.decode(TokenIDs[event_pos[0]:event_pos_end[0]]))\n",
    "            print(\"second event of the pair:\", tokenizer.decode(TokenIDs[event_pos[1]:event_pos_end[1]]))\n",
    "            print(\"TokenIDs:\", tokenizer.decode(TokenIDs))\n",
    "        \n",
    "        if params['nd']:\n",
    "            syn_0 = replace_with_syn(tokenizer.decode(TokenIDs[event_pos[0]:event_pos_end[0]]))\n",
    "            syn_1 = replace_with_syn(tokenizer.decode(TokenIDs[event_pos[1]:event_pos_end[1]]))\n",
    "            if len(syn_0) > 0:\n",
    "                TokenIDs = TokenIDs[0:event_pos[0]] + tokenizer.encode(syn_0[0])[1:-1] + TokenIDs[event_pos_end[0]:]\n",
    "                prev = event_pos_end[0]\n",
    "                event_pos_end[0] = event_pos[0] + len(tokenizer.encode(syn_0[0])[1:-1])\n",
    "                if prev != event_pos_end[0]:\n",
    "                    offset = event_pos_end[0] - prev\n",
    "                    event_pos[1] += offset\n",
    "                    event_pos_end[1] += offset\n",
    "            if len(syn_1) > 0:\n",
    "                TokenIDs = TokenIDs[0:event_pos[1]] + tokenizer.encode(syn_1[0])[1:-1] + TokenIDs[event_pos_end[1]:]\n",
    "                prev = event_pos_end[1]\n",
    "                event_pos_end[1] = event_pos[1] + len(tokenizer.encode(syn_1[0])[1:-1])\n",
    "            #assert 1 == 0\n",
    "        feature = {'input_ids': TokenIDs,\n",
    "                   'event_pos': event_pos,\n",
    "                   'event_pos_end': event_pos_end,\n",
    "                   'event_pair': [[1, 2]],\n",
    "                   'labels': relations,\n",
    "                  }\n",
    "        if file_name in onlyfiles_TB:\n",
    "            features_train.append(feature)\n",
    "        elif file_name in onlyfiles_AQ:\n",
    "            features_valid.append(feature)\n",
    "        elif file_name in onlyfiles_PL:\n",
    "            features_test.append(feature)\n",
    "            test_labels.append(xy)\n",
    "            timeline_input.append([fname, x, y, xy])\n",
    "    if debug:\n",
    "        break\n",
    "        \n",
    "elapsed = format_time(time.time() - t0)\n",
    "print(\"MATRES Preprocessing took {:}\".format(elapsed)) \n",
    "print(\"Temporal Relation Stats:\", relation_stats)\n",
    "print(\"Total num of pairs:\", pair_num)\n",
    "print(\"Max length of context:\", max_len)\n",
    "print(\"Avg num of sentences that context contains:\", sent_num/pair_num)\n",
    "print(\"Context length stats(unit: sentence): \", context_len)\n",
    "print(\"MATRES train valid test pair num:\", len(features_train), len(features_valid), len(features_test))\n",
    "#with open(\"MATRES_test_timeline_input.json\", 'w') as f:\n",
    "#    json.dump(timeline_input, f)\n",
    "#    assert 0 == 1\n",
    "    \n",
    "#output_file = open('test_labels.txt', 'w')\n",
    "#for label in test_labels:\n",
    "#    output_file.write(str(label) + '\\n')\n",
    "#output_file.close()\n",
    "#if debug:\n",
    "#    assert 0 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4c9376c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(features_test, batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mfeatures_train\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m     valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(features_valid, batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(features_test, batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    train_dataloader = DataLoader(features_train, batch_size=params['batch_size'], shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "    valid_dataloader = test_dataloader = train_dataloader\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        print(batch)\n",
    "elif params['testdata'] == 'TDD':\n",
    "    features_valid, labels_valid, labels_full_valid = TDD_processor('man-dev')\n",
    "    features_test, labels_test, labels_full_test = TDD_processor('man-test')\n",
    "    #print(len(labels_test)) # man_test, 846\n",
    "    #print(abnormal_articles) # CNN19980213.2130.0155\n",
    "    #with open(\"tdd_labels.json\", 'w') as f:\n",
    "    #    json.dump(labels_full_test, f)\n",
    "    #with open(\"tdd_labels.txt\", 'w') as f:\n",
    "    #    for label in labels_test:\n",
    "    #        print(label, file = f)\n",
    "    valid_dataloader = DataLoader(features_valid, batch_size=params['batch_size'], shuffle=False, collate_fn=collate_fn, drop_last=False) \n",
    "    test_dataloader = DataLoader(features_test, batch_size=params['batch_size'], shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "elif params['testdata'] == 'PRED':\n",
    "    test_dataloader = DataLoader(features_test, batch_size=params['batch_size'], shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "else:\n",
    "    train_dataloader = DataLoader(features_train, batch_size=params['batch_size'], shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "    valid_dataloader = DataLoader(features_valid, batch_size=params['batch_size'], shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "    test_dataloader = DataLoader(features_test, batch_size=params['batch_size'], shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "print(\"  Data processing took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70cd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f48f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
